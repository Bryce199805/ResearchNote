# 20240621


---

## 论文阅读  

> 这个结构给了CNN建模长程依赖关系的能力，但其能否与ViT中自注意力的长程依赖特征对齐还有待验证
### GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond  2019ICCV
- 将全局特征计算出来叠加到所有位置来获得长程依赖关系
- 全局特征与查询位置无关，一次计算对所有位置应用，计算过程称之为全局注意力池化，用来上下文建模
- 使用瓶颈网络来减少参数的数量，使之轻量化，瓶颈变换能够捕捉通道间的依赖关系
- 这项工作是针对NLnet的改进，计算开销更小但有着相似的性能

---

## 论文写作

更新后的目录和进度：

1. 引言 **未完成**
2. 知识蒸馏的背景  **补充了ViT的符号系统**
3. 知识的类型  **前三节现有综述有  第四节现有综述没有**
   1. 基于响应的蒸馏  **已完成** 
      1. 预测响应
      2. 特征响应
   2. 基于特征的蒸馏  **已完成**
   3. 基于关系的蒸馏  **已完成**
   4. Transformer架构的蒸馏  **基本完成**
      1. Transformer架构向卷积架构蒸馏
      2. 卷积架构向Transformer架构蒸馏
      3. Transformer架构向Transformer架构蒸馏
4. 知识蒸馏的优化  **未完成**
   1. 蒸馏的解耦
   2. 温度系数的研究
5. 知识蒸馏的类型  **未完成** **这一章还要不要？**
   1. 在线蒸馏
   2. 自蒸馏
   3. 数据集蒸馏
6. 知识蒸馏的应用  **未完成**
7. 总结与展望   **未完成**
   

---

## 代码实践

暂时还没开始写

---

## 项目进展

- 研究了cae后端go代码

---


