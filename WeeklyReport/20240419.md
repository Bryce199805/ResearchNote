# 20240419

---

## 论文阅读

> Feature
### Knowledge Condensation Distillation 2022ECCV

### Bag of Instances Aggregation Boosts Self-Supervised Distillation 2022ICLR

### Knowledge Distillation via the Target-aware Transformer 2022CVPR

---

## 代码实践

#### 2023AAAI CTKD
引入了对抗的思想，因此有最大化损失的地方，引入了一个梯度反转层：
梯度反转层的反转再计算lambda时实现，直接对原lambda取负后拿过来计算后续，从而实现最大化的操作
继承Function类来重写 forward() backward()方法来实现，将梯度按照我们想要的方式来处理
```python
### CORE CODE
### ctx是一个上下文对象，可以保存forward过程中的值供backward过程调用来节约开销
class GradientReversalFunction(Function):
    @staticmethod
    def forward(ctx, x, lambda_):
        ctx.lambda_ = lambda_
        return x.clone()
    @staticmethod
    def backward(ctx, grads):
        lambda_ = ctx.lambda_
        lambda_ = grads.new_tensor(lambda_)
        dx = lambda_ * grads
        return dx, None

'''
To create a custom `autograd.Function`, subclass this class and implement
the :meth:`forward` and :meth`backward` static methods. Then, to use your custom
op in the forward pass, call the class method ``apply``. Do not call
:meth:`forward` directly.
'''
# !在调用时不能直接使用 backward or forward 要使用apply方法
# such as 
class GradientReversal(torch.nn.Module):
    def __init__(self):
        super(GradientReversal, self).__init__()
    def forward(self, x, lambda_):
        return GradientReversalFunction.apply(x, lambda_)

```
[refs](https://pytorch.org/docs/stable/notes/extending.html)
---

## 项目进展

---

## Mathematics
