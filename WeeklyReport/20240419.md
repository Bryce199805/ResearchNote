# 20240419

---
对于先前比较经典高引用量的蒸馏文献基本已经阅读完毕,下个周我会对目前读过的文献做一个梳理总结.
早期的文章倾向于Feature中间层特征的迁移,而近期的文章转向对于logits的更好迁移,以及整个蒸馏过程的优化.关于优化蒸馏过程这一角度目前工作较少,这可能是我未来工作的一个潜在的切入点.

## 论文阅读

> Feature 这几项工作都是针对特征图相似性展开的工作
### Exploring Inter-Channel Correlation for Diversity-preserved Knowledge Distillation 2021ICCV
这项工作指出先前的方法没有关注到特征通道之间的相似性，提出ICC通道间相关性，让学生模型去学习模仿教师模型通道间的相似性和多样性，对于分类任务等较小的特征图，直接对每个通道的特征图拉平计算内积得到相似度矩阵，针对密集预测等较大的特征图，提出了分块蒸馏策略，特征图进行分块分别计算相似性，然后对每个块的相似性信息进行聚合。

### Bag of Instances Aggregation Boosts Self-Supervised Distillation 2022ICLR (code)
这项工作提出将相似的样本装入袋中进行蒸馏来教授学生模型的无监督对比学习蒸馏框架，通过知识丰富的预训练无监督教师模型对数据集进行特征提取，根据特征相似性对数据集进行分组，将相似的数据装入一个袋子中，每个袋子中都有一个锚点，其他样本是与该数据对比来衡量相似性。分好袋后学生模型以袋数据为基础进行蒸馏学习，提出样本内蒸馏损失和样本间蒸馏损失，前者将同一样本的不同增强下的距离拉近，后者将同一袋子中的不同样本距离拉近，以此来指导学生模型学习。

### Knowledge Distillation via the Target-aware Transformer 2022CVPR (code)
这项工作针对先前工作中特征图一对一匹配的语义不匹配问题给出解决方案，首先提出了Target-aware Transformer(TaT)，利用教师特征对学生的语义特征进行重建，经过变换的学生特征能够在语义上于对应位置的教师特征保持一致，对于大特征图这个步骤复杂度太高，提出了分组蒸馏和锚点蒸馏，前者将特征图切块进行蒸馏得到局部特征，后者对特征图池化，得到较小的特征图进行蒸馏，得到全局特征，两者相辅相成最终达到不错的性能。

### Hierarchical Self-supervised Augmented Knowledge Distillation 2021IJCAI
这项工作认为基于对比学习的蒸馏会损害模型对原任务表征的学习性能，将先前工作提出的用于自蒸馏的联合标签分布增强应用到蒸馏中，并提出一个分组机制将教师模型在增强数据的联合分布输出教授给学生模型。
*将2020PMLR提出的自蒸馏增强机制用到了传统的蒸馏框架中。*

---

> Features 近期工作发现师生模型差距较大时会导致学生模型的性能不佳,这两项工作对这一问题进行了讨论
### Knowledge Distillation from A Stronger Teacher 2022NeurIPS (code)
这项工作针对师生模型差距较大时KD优化性能不良的问题进行了讨论，学生模型推理时关注的是教师输出的相对排名而不是确切的概率值，因此本文提出引入样本和类别之间的类内相似性和类间相似性，并且只针对排名而不需要完全相同使用KL散度并不合理，引入皮尔森系数作为新的距离度量。

### Densely Guided Knowledge Distillation using Multiple Teacher Assistants 2021ICCV
本文针对师生模型差距较大时学生模型训练不良的问题进行了研究，主要是针对2020AAAI提出的TAKD引入助教来弥合差距这一方法的改进，TAKD有一个问题就是其辅助模型判错时这个错误会一直累计下去造成学生模型的性能不佳，本文借鉴DenseNet的设计思想提出了DGKD，用先前所有的TA和教师模型对学生模型进行蒸馏学习缓解这一问题，并借鉴Dropout的思想对TA进行随机丢弃来缓解过拟合的问题。

---

> Optimistic  这两项工作从优化的角度对蒸馏进行研究
### Knowledge Condensation Distillation 2022ECCV
这项工作认为对于全部的知识进行蒸馏是冗余的，学生模型不应该被动的接收来自教师的所有知识，提出了在线全局价值估计来计算每个知识点对于学生模型的价值，自适应知识总结来获得教师模型中更精简有效的知识编码，将知识划分为三部分，对强知识直接蒸馏，对弱知识直接丢弃，中等知识则用弱知识增强后在蒸馏，由此更高效的对模型进行训练。

### Self-Distillation from the Last Mini-Batch for Consistency Regularization 2022CVPR
这项工作提出了一种新的自蒸馏方法，来解决先前方法丢失即时信息、消耗额外内存、并行化程度低的问题，提出了利用上一次miniBatch输出结果进行蒸馏的方法，在每个批量上一半与上一次的结果保持一致，另一半与下一次的结果保持一致来保持模型的一致性

---

> 这项工作进行了大量的实验,来研究蒸馏效果不好的原因,最终得出是因为优化的问题,近期有少量工作是关于优化问题的改进,效果都很显著,我认为这可能是我日后工作的一个潜在的切入点
### Does Knowledge Distillation Really Work? 2021NeurIPS
这项工作通过大量实验来探讨蒸馏效果不佳的原因，对可能的原因逐一实验排查，最终发现是由于优化方法限制了模型的收敛，在最后总结了本文的实验发现。

---

## 代码实践

#### 2023AAAI CTKD
引入了对抗的思想，因此有最大化损失的地方，引入了一个梯度反转层：
梯度反转层的反转再计算lambda时实现，直接对原lambda取负后拿过来计算后续，从而实现最大化的操作
继承Function类来重写 forward() backward()方法来实现，将梯度按照我们想要的方式来处理
```python
### CORE CODE
### ctx是一个上下文对象，可以保存forward过程中的值供backward过程调用来节约开销
class GradientReversalFunction(Function):
    @staticmethod
    def forward(ctx, x, lambda_):
        ctx.lambda_ = lambda_
        return x.clone()
    @staticmethod
    def backward(ctx, grads):
        lambda_ = ctx.lambda_
        lambda_ = grads.new_tensor(lambda_)
        dx = lambda_ * grads
        return dx, None

'''
To create a custom `autograd.Function`, subclass this class and implement
the :meth:`forward` and :meth`backward` static methods. Then, to use your custom
op in the forward pass, call the class method ``apply``. Do not call
:meth:`forward` directly.
'''
# !在调用时不能直接使用 backward or forward 要使用apply方法
# such as 
class GradientReversal(torch.nn.Module):
    def __init__(self):
        super(GradientReversal, self).__init__()
    def forward(self, x, lambda_):
        return GradientReversalFunction.apply(x, lambda_)

```
[refs](https://pytorch.org/docs/stable/notes/extending.html)

#### 2022NeurIPS DIST
研究了皮尔森系数以及皮尔森距离的实现，另外针对torch中张量维度的传递之前一直很混乱，捋了一下在模型中以及损失计算中张量维度的变化。

#### 2022CVPR TaT   2022ICLR BINGO
简单看了一下各自提出的模块和损失函数的实现.

---

## 项目进展
正在用c++重写go语言的后端处理逻辑。
mongodb数据库的c++驱动依赖问题很大，在这里浪费了很多时间。

---

## Mathematics
对Jensen不等式进行了证明推导，并利用其证明KL散度非负

### Convex Function 

>  特别指出，凸函数定义在国内某些教材上与国际上相反，以国际标准为准。

凸函数指函数图像上，任意两点连成的线段，皆位于图形上方的实值函数。形式化定义：

实值函数$f:C \rightarrow \R, \forall t\in[0, 1], \forall v, w\in C$:
$$
f[v + t·(w-v)] \leq f(v) - t·[f(w) - f(v)] \tag{1}
$$
 则$f$称为凸函数



### Jensen's Inequality

假设$f:I \rightarrow \R$为凸函数，则$\forall x_1, ..., x_n\in I, \forall t_1, ..., t_n\in[0, 1], t_1+t_2+...+t_n=1$, 有：
$$
f(t_1x_1+...+t_nx_n) \leq t_1f(x_1)+...+t_nf(x_n) \tag{2}
$$

#### Proof.

由数学归纳法：
$$
\begin{aligned}
& n=1:f(t_1x_1) \leq t_1f(x_1) &
\end{aligned}
\tag{3}
$$

$$
\begin{aligned}
& n=2:f(t_1x_1+t_2x_2) \leq t_1f(x_1) + t_2f(x_2) &
\end{aligned}
\tag{4}
$$

由凸函数定义上式显然成立。

假设：$n=k: \leq t_1f(x_1)+t_2f(x_2)+...+t_kf(x_k)$成立:
$$
\begin{aligned}
& n = k+1: &\\
& f(t_1x_1 + t_2x_2+...+ t_kx_x + t_{k+1}x_{k+1}) = f[(1-t_{k+1})\frac{t_1x_1 + t_2x_2+...+t_kx_x}{(1-t_{k+1})} + t_{k+1}x_{k+1}]
\end{aligned} \tag{5}
$$
由Eq(3), Eq(4):
$$
\begin{aligned}
& f[(1-t_{k+1})\frac{t_1x_1 + t_2x_2+...+t_kx_x}{(1-t_{k+1})} + t_{k+1}x_{k+1}]  &\\
& \leq (1-t_{k+1}) f(\frac{t_1}{1-t_{k+1}}x_1 + \frac{t_2}{1-t_{k+1}}x_2 + ... + \frac{t_k}{1-t_{k+1}}x_k) + t_{k+1}x_{k+1} \\
& \leq t_1f(x_1) + t_2f(x_2) + ... + t_kf(x_k) + t_{k+1}x_{x+1}
\end{aligned} \tag{6}
$$
得证。



#### Probability Theory Version

对随机变量$X$, $\varphi$为凸函数：
$$
\varphi(E(X)) \leq E(\varphi(X)) \tag{7}
$$
即：

对$\int^\infty_{-\infty}f(x)dx = 1$，$\varphi$在$g$的值域上为凸函数：
$$
\varphi(\int^\infty_{-\infty}g(x)f(x)dx) \leq \int^\infty_{-\infty}\varphi(g(x))f(x)dx \tag{8}
$$
对$\Omega = \{x_1, x_2, ..., x_n\}, \sum_{i=1}^n\lambda_i=1, \lambda_i \geq0$：
$$
\varphi(\sum_{i=1}^ng(x_i)\lambda_i) \leq \sum^n_{i=1}\varphi(g(x_i))\lambda_i \tag{9}
$$

### 证明KL散度非负

对离散随机变量$\xi$，存在两个概率分布$P,Q$，则KL散度定义为：
$$
KL(P||Q) = \sum_iP(i)log\frac{P(i)}{Q(i)} \\
\sum_iP(i) = \sum_iQ(i) = 1 \tag{10}
$$

#### Proof.

$$
\sum_iP(i)log\frac{P(i)}{Q(i)} = \sum_iP(i)[-log\frac{Q(i)}{P(i)}] \geq -log[\sum_iP(i)\frac{Q(i)}{P(i)}] = -log\sum_iQ(i)=0 \\
KL(P||Q) \geq 0
\tag{11}
$$

