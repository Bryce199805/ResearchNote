# 20240405

---

## 论文阅读
这周主要看了较新的文献和其中反复出现的旧文献，在20-22年的文献目前有个断层很多内容接不上，正在弥补
> Feature 以下几项是关于特征蒸馏的工作
> 这一组工作利用了注意力图来进行特征的转移
### Learning Deep Features for Discriminative Localization 2016CVPR (code)
这项工作对于先前工作提出的全局平均池化(GAP)稍作调整，提出了类别注意力图(CAM)，使其能够识别网络在对于某个类别的判别区域

### Class Attention Transfer Based Knowledge Distillation 2023CVPR (code)
这项工作根据先前工作提出的类别注意力图(CAM 2016CVPR)，提出了类别注意力转换(CAT)调整CAM的生成位置，使之可以作为蒸馏知识教授学生模型，这种方法不仅具有更好的性能还具有更好的可解释性。

### Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer 2017ICLR-0322

---
> 这两项工作以及logit蒸馏的BANs是旧工作中出现频率比较高的经典工作
### Contrastive Representation Distillation 2020ICLR (code)
传统的知识蒸馏将所有目标维度视为互相独立的，而这项工作认为他们之间存在着依赖关系，而传统的蒸馏范式无法传递这种依赖，这项工作希望引入了对比损失，希望捕获相关性和高阶输出之间的依赖关系，通过最大化教师和学生表示之间的互信息的下界来达到这一目的。

### Distilling Knowledge via Knowledge Review 2021CVPR
先前工作都集中在使用相同级别的信息来指导学生进行学习，而忽略了跨层级的知识，本文提出一种新的学习框架，让学生的高层去学习教师模型的低层特征，并且引入了注意力融合ABF来完成对于教师模型从低层到高层特征的融合，并引入层级上下文损失HCL来完成到学生高层的知识传递

---

> Logits 从此处往下是关于logits蒸馏的工作
> 这一组工作探究了logits蒸馏性能不好的原因，通过传递更多知识解决这一问题
### Born Again Neural Networks 2018PMLR
本文提出一个序列化的建模过程，令前一个模型为后一个模型的老师，依次来进行序列化的学习，最后将所有的学生模型集成成一个模型作为最终的输出。

### Multi-level Logit Distillation 2023CVPR 
这项工作进一步研究了关于logit的蒸馏，先前的方法仅仅是对实例级的师生特征进行对齐，我们认为这是不够的，我们提出一种多层次的对齐方式，在实例级、批次级和类别级对教师模型进行对齐，通过计算批次中样本间相似性、类别间相似性来帮助学生学习

---

> 这几项工作认为师生模型差距较大时会影响蒸馏的性能，通过优化学生架构来获得更好的性能
### DisWOT: Student Architecture Search for Distillation Without Training 2023CVPR
针对师生模型差距大的情况下蒸馏性能不佳的问题，给出了一种基于进化算法的学生模型搜索策略，考虑了类别和样本之间的相似性，通过免训练的搜索架构得到最优的学生架构，在与预定义的教师模型进行蒸馏学习。  
这项工作其中的两个关键点来自于：  
*Learning Deep Features for Discriminative Localization 2016CVPR*  
*Similarity-Preserving Knowledge Distillation 2019ICCV*  
**这篇有几个地方还没弄懂，尤其是关于搜索算法这块，需要结合先前文献和代码在探究**

### On the Efficacy of Knowledge Distillation 2019ICCV
这项工作进行了大量的实验说明，发现先前的蒸馏方法教师并不是最优的教师，学生的学习也不是最优的情况。我们发现在施加蒸馏损失的学习中，在后期不如直接训练，任务损失与蒸馏损失产生冲突，我们提前停止蒸馏来缓解这一问题，后期仅用任务损失进行优化；针对高精度的大的教师模型，我们认为教师模型给出的解根本不在较小的学生模型的解空间中，我们需要找到一个在其解空间中的解，结合先前工作经过少量训练的大的教师模型的搜索空间仍然比小的模型大，我们对教师模型也施加提前停止。本文的很多思想在***2023CVPR DOT***中很明显的体现，后者认为是对损失没有做好最佳的优化而导致的性能不足并从优化的角度解决了这一问题，同样带来了性能的提升。

### Improved Knowledge Distillation via Teacher Assistant 2020AAAI-0315

---

> 这两项工作认为logits蒸馏性能不佳是由于优化不足所导致的，这组的工作其实与上一组的很相似，上一组更多是在模型架构的视角审视解决这一问题，这一组是从优化的角度来解决这一问题。这两项工作是出自同一作者。
### Decoupled Knowledge Distillation 2022CVPR 
这项工作从研究关于logit的蒸馏,logit的语义级别高于深度特征,作者认为传统的kd损失在logit蒸馏上效果不佳是因为受到了某些未知因素的抑制,因此这项工作重新审视传统的蒸馏,并将其进行解耦,以破除这种对于性能的限制. 

### DOT: A Distillation-Oriented Trainer 2023ICCV (code)
这项工作发现在引入蒸馏损失后并没有如期下降，认为是任务损失与蒸馏损失发生冲突，任务损失和蒸馏损失简单相加可能会导致优化方式退化为多任务学习，而这项工作认为打破这种情况的关键是充分优化蒸馏损失，提供分别调整任务损失和蒸馏损失的动量值来让蒸馏占到主导地位，从而使两种损失都充分收敛达到更好的性能。


--- 

> 应用 利用蒸馏来做增量学习的一项工作
### DKT: Diverse Knowledge Transfer Transformer for Class Incremental Learning 2023CVPR
这项工作介绍了一种增量学习的方法，设计了一个token pool，包括通用知识token和特定任务token，分别用来存储任务的一般知识和任务专有的知识，并设计了两个自注意力模块GKAB和SKAB，分别用来维护通用知识和专用知识，引入两个分类器可塑性分类器和稳定性分类器，分别用来识别新类和旧类，并且提出一个聚类分离损失来帮助其进行训练。

---

## 代码实践
复现了上述标记code文献的代码

### CAM_2016CVPR：
提到一个钩子函数，来自nn.Module的类方法，可以直接拿到在训练和推理过程中中间层的特征图

### CRD_2020ICLR：
在蒸馏中引入了对比学习的思想，对比学习损失和采样流程实现的比较复杂，还没有完全看懂

### CAT-KD_2023CVPR：
主要研究了作者对模型结构的修改的实现，以及复杂的损失函数的实现

### DOT_2023ICCV： 
这项工作提出新的优化方式，在代码中构建了一个类继承了Optimizer，重写了其step方法，自定义我们的梯度优化过程
