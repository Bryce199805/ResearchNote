# 20240315
## 论文阅读
### Do Deep Nets Really Need to be Deep? 2014NeurIPS
### Distilling the Knowledge in a Neural Network 2014NeurIPS    

---

### Like What You Like: Knowledge Distill via Neuron Selectivity Transfer 2017arXiv
### FitNets: Hints For Thin Deep Nets 2015ICLR
### Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer 2017ICLR

---

