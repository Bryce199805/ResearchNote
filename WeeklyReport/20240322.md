# 20240322
## 论文阅读
本周阅读知识迁移相关文献，更多的关注文献间的关联以及一些理论性的分析

### Do Deep Nets Really Need to be Deep? 2014NeurIPS
### Distilling the Knowledge in a Neural Network 2014NeurIPS    
第一篇提出了使用软标签来训练学生网络，并提出了温度系数的概念来加强概率较低的标签概率对于模型的影响；第二篇提出了一种新的训练网络的方式，先前利用原始数据来训练网络无法在浅层网络上达到较高的精度，但模型压缩告诉我们较小的网络也能够达到与复杂模型相似的精度，因此提出简单网络模仿已经训练好的复杂网络的logit输出，来达到简单网络实现与深度网络相似的精度，并且在隐藏层的学习中引入了矩阵分解来加快模型的学习。

> 两篇为同年工作，都属于是蒸馏学习的开端工作，二者思想其实一致都是想利用软标签来训练网络，而有些概率较低的标签经过softmax后对于模型影响力甚微，前者通过温度系数解决这一问题而后者直接绕开softmax对于这些较低概率值的在缩小。  

---

这一组是关于Knowledge Transfer一些比较相关的文献
### FitNets: Hints For Thin Deep Nets 2015ICLR
不直接对齐师生模型的特征图，而是训练一个比教师网络更深但更细的网络来进行模型压缩，因为有研究表明更深的网络的表达能力要比更浅的网络高指数倍，只需要合适的训练方式。  

> FitNets是首个提出不直接对齐师生模型完成特征图的工作，后续很多研究是基于这一思想而继续进行的。

### Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer 2017ICLR
与传统的基于网络的完全激活进行知识转移不同，这项工作给出了一种基于注意力图方式的知识转移，给出了两种不同的注意力图的定义方式并给出了对应的AT损失。 

> AT提出了对神经网络中间层的注意力图进行转移，这也推动了后续转移知识的多样化

### Like What You Like: Knowledge Distill via Neuron Selectivity Transfer 2017arXiv
这项工作提出了一种新的知识转移方法，通过最小化师生模型特征图分布之间的最大均值差异（MMD）来匹配教师网络和学生网络中选择性模式的分布，通过不同的核函数来获得不同的损失计算，NST是对现有方法的补充，与KD相结合达到了更好的性能效果，进一步证明知识转移能够学习到更好的特征表示。  

### Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons 2019AAAI (code)
基于先前工作的启发，作者认为激活信息是非常重要的，先前工作集中于对于激活值的传递，而我们认为对于激活边界的传递是更为重要的，因此修改损失函数来传递教师模型的激活边界信息，而这个修改导致了损失函数的离散化，无法进行梯度下降优化，为了解决这个问题引入了一个近似函数，将教师模型的神经元激活值视为二分类中的标签，转化为连续的二分类问题，在性能上取得了提升。 

> NST 与 Distillation of Activation Boundaries 都关注于教师网络的激活信息，而前者所学习到的激活边界会对弱响应不敏感，后者强化了对于弱响应的神经元从而能够从教师模型学到一个完整的激活边界。

### DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer 2018AAAI  
先前方法都仅考虑单样本的知识，都没有考虑不同样本之间的关系，这项工作引入知识转移的交叉样本相似性，提出了一套对样本进行相似度打分的分数损失，利用样本间的知识对学生模型进行训练。     

> DarkRank 与 NST 模型架构完全相同，DarkRank提出学习样本之间的相似性来帮助学习，后者则学习教师模型激活的神经元分布

### Paraphrasing Complex Network: Network Compression via Factor Transfer 2018NeurIPS (code)
这项工作认为先前方法都忽略了师生模型之间的内在差异，简单的提供教师知识而没有任何解释，不利于学生模型的理解，因此提出了因子转移的方法，在教师网络上添加一个释义器，来解释教师模型的特征图；学生网络上添加一个翻译器，来帮助学生模型理解教师模型释义器的内容。学生模型通过与标签的交叉熵和释义器-翻译器的知识转移共同训练。  

> 这项工作引入了辅助网络来帮助师生模型学习，这在之前的文献从未出现（起码我没看到）

---
这两项工作是对于数据的操作，也达到了不错的效果

### Adapting Models to Signal Degradation using Distillation 2017BMVC CCF-C
这项工作指出训练时能够获得高质量的数据但在测试推理时可能并不能保证都是高质量的，例如一个分类器训练时是高分辨率的图像，但是在测试时用户上传的时质量很低的图像，针对这种问题本文提出了CQD交叉质量蒸馏，主要思想是给定高质量样本训练一个教师模型，然后对高质量样本做一个变换降低其质量，训练一个学生模型，利用知识蒸馏让学生模型去模仿教师模型，从而达到输入低质量图像也能达到一个不错的性能效果。  


### Data Distillation: Towards Omni-Supervised Learning 2018CVPR
提出了一种新的学习方法，称之为数据蒸馏，利用已标记的数据训练一个模型，对未标记的数据做数据变换，计算出一个标签后进行聚合，然后与原本的标记数据合并再继续训练该模型，延长训练计划，通过这种方式能够超过大规模的监督学习。

---

> 这两项工作也是关于Knowledge Transfer的研究，但这两篇都有比较强的数学理论推导，我试着推导了以下。
> 对于这类实践我称之为培养从"1->10"的能力，至于如何培养自身从"0->1"的能力，还需做更深刻的思考。
> 第一篇的理论分析是从数学分析近似的角度来讨论的；第二篇则是从信息论的视角分析了所提出的方法

### Local Affine Approximations for Improving Knowledge Transfer 2018Idiap?
本文提出了通过雅可比矩阵的匹配来进行师生模型之间的知识转移，本文认为两个网络的雅可比矩阵匹配等价于在训练过程中对输入添加噪声的软标签进行匹配，通过泰勒公式对神经网络进行仿射逼近，验证了这一观点。本次周报最后我给出了完整的证明过程。  

### Learning Deep Representations with Probabilistic Knowledge Transfer 2018ECCV
本文提出一种利用概率的方法来让学生模型模仿教师模型的特征空间，对数据样本之间成对交互进行建模可以描述相应特征空间的几何形状，因此使用模型特征空间中样本间的联合概率密度来描述特征空间的形状，联合概率密度通过核密度估计来计算，最后还从信息论的角度分析了本文方法的有效性。

---

## 代码实践
复现了上述论文中后用***code***标识的文章代码  
对于上周遗留的torchnet.Engine模型训练框架问题，本周进行了实践
对于上周遗留的NNI微软调参框架，本周进行了实践，实践过程中发现该框架有很多众网友反应的不明错误至程序崩溃，弃用

---

## 项目进展
研究了关于Kubesphere go语言后端的项目  
对于docker的相关操作进行了学习实践  

---

## Proofs
根据一阶泰勒公式，对函数$f:\mathbb{R}^D \rightarrow \mathbb{R}$，在一个邻域内$\{ x+\Delta x:||\Delta x||\le \epsilon \}$：
$$
f(x+\Delta x)=f(x) + \nabla_xf(x)^T(\Delta x) + \mathcal{O}(\epsilon) \approx f(x) + \nabla_xf(x)^T(\Delta x) 
$$

给定问题：

***在一个数据集$\mathcal{D}$上训练教师网络$\mathcal{T}$，用$\mathcal{T}$的提示来增强学生网络$\mathcal{S}$在$\mathcal{D}$上的训练。***


我们认为**两个网络的雅可比矩阵匹配等价于在训练过程中对输入添加噪声的软标签进行匹配。**


### Proposition 1.

考虑两个k维度（$\in \mathbb{R}^k$）的神经网络软标签的平方损失误差：$\mathcal{l}(\mathcal{T}(x), \mathcal{S}(x)) = \sum^k_{i=1}(\mathcal{T}^i(x) - \mathcal{S}^i(x))$，$x\in \mathbb{R}^D$为输入数据，令$\xi(\in \mathbb{R}^D) = \sigma z$为缩放系数为$\sigma\in\mathbb{R}$的标准正态分布随机变量，因此有以下成立：
$$
\mathbb{E}_{\xi}[\sum^k_{i=1}(\mathcal{T}^i(x+\xi) - \mathcal{S}^i(x+\xi))^2] = \sum_{i=1}^k(\mathcal{T}^i(x)-\mathcal{S}^i(x))^2 + \sigma^2\sum^k_{i=1}||\nabla_x\mathcal{T}^i(x)-\nabla_x\mathcal{S}^i(x)||^2_2 + \mathcal{O}(\sigma^4)
$$
该式子将损失分解为两部分，一个代表样本上的蒸馏损失，另一项代表雅可比匹配损失，当$\sigma$较小时，最终误差项很小可以忽略。

这种推论也能推广到交叉熵误差上：
$$
\mathbb{E}_{\xi}[-\sum^k_{i=1}\mathcal{T}^i_s(x+\xi)log(\mathcal{S}^i_s(x+\xi))] \approx -\sum_{i=1}^k\mathcal{T}^i_s(x)log(\mathcal{S}^i_s(x)) - \sigma^2\sum^k_{i=1}\frac{\nabla_x\mathcal{T}^i_s(x)^T \nabla_x\mathcal{S}^i_s(x)}{\mathcal{S}^i_s(x)}
$$

#### Proofs 1.

对$\mathcal{T}(x+\xi),\mathcal{S}(x+\xi)$做一阶泰勒展开，代入有：
$$
\mathbb{E}_{\xi}[\sum^k_{i=1}(\mathcal{T}^i(x+\xi) - \mathcal{S}^i(x+\xi))^2] = \mathbb{E}_{\xi}[\sum^k_{i=1}(\mathcal{T}^i(x) + \xi\nabla_x\mathcal{T}^i(x) - \mathcal{S}^i(x) - \xi\nabla_x\mathcal{S}^i(x))^2] + \mathcal{O}(\sigma^4) \\
= \mathbb{E}_{\xi}[\sum^k_{i=1}(\mathcal{T}^i(x) - \mathcal{S}^i(x))^2 + \sum^k_{i=1}\xi^2(\nabla_x\mathcal{T}^i(x)  - \nabla_x\mathcal{S}^i(x))^2 + \sum^k_{i=1}(2\xi ( \mathcal{T}^i(x) - \mathcal{S}^i(x) )( \nabla_x\mathcal{T}^i(x)  - \nabla_x\mathcal{S}^i(x) )
] + \mathcal{O}(\sigma^4) \\
= \mathbb{E}_{\xi}[\sum^k_{i=1}(\mathcal{T}^i(x) - \mathcal{S}^i(x))^2] + \mathbb{E}_{\xi}[\sum^k_{i=1}\xi^2(\nabla_x\mathcal{T}^i(x)  - \nabla_x\mathcal{S}^i(x))^2] + \mathbb{E}_{\xi}[\sum^k_{i=1}(2\xi ( \mathcal{T}^i(x) - \mathcal{S}^i(x) )( \nabla_x\mathcal{T}^i(x)  - \nabla_x\mathcal{S}^i(x) )] + \mathcal{O}(\sigma^4) \\
= \sum_{i=1}^k(\mathcal{T}^i(x)-\mathcal{S}^i(x))^2 + \sigma^2\sum^k_{i=1}||\nabla_x\mathcal{T}^i(x)-\nabla_x\mathcal{S}^i(x)||^2_2 + \mathcal{O}(\sigma^4)
$$
$\xi = \sigma z$，z为标准正态分布，因此有$\mathbb{E}_{\xi}[\xi]=0,\mathbb{E}_{\xi}[\xi^2]=\sigma^2$.  

对交叉熵损失，$\mathcal{T}(x+\xi),\mathcal{S}(x+\xi)$做一阶泰勒展开，代入有：

$$
\mathbb{E}_{\xi}[-\sum^k_{i=1}\mathcal{T}^i_s(x+\xi)log(\mathcal{S}^i_s(x+\xi))] 
= \mathbb{E}_{\xi}[-\sum^k_{i=1} (\mathcal{T}^i_s(x) + \xi\nabla_x\mathcal{T}^i_s(x)) log(\mathcal{S}^i_s(x)+\xi\nabla_x\mathcal{S}^i_s(x))]
$$

对$log(\mathcal{S}^i_s(x)+\xi\nabla_x\mathcal{S}^i_s(x))$做二阶泰勒展开有：

$$
log(\mathcal{S}^i_s(x)+\xi\nabla_x\mathcal{S}^i_s(x)) \approx log[\mathcal{S}^i_s(x)] + \frac{\xi\nabla_x\mathcal{S}^i_s(x)}{\mathcal{S}^i_s(x)} - \frac{1}{2}\frac{\xi^2\nabla^2_x\mathcal{S}^i_s(x)}{[\mathcal{S}^i_s(x)]^2}
$$

代入有：

$$
\approx \mathbb{E}_{\xi}[-\sum^k_{i=1} (\mathcal{T}^i_s(x) + \xi\nabla_x\mathcal{T}^i_s(x)) (log\mathcal{S}^i_s(x)+\frac{\xi\nabla_x\mathcal{S}^i_s(x)}{\mathcal{S}^i_s(x)}-\frac{1}{2}\frac{\xi^2\nabla^2_x\mathcal{S}^i_s(x)}{[\mathcal{S}^i_s(x)]^2})] \\
=\mathbb{E}_{\xi}[-\sum^k_{i=1} [(\mathcal{T}^i_s(x) log\mathcal{S}^i_s(x)) (\xi \mathcal{T}^i_s(x) \frac{\nabla_x\mathcal{S}^i_s(x)}{\mathcal{S}^i_s(x)}) (-\frac{\xi^2}{2}\mathcal{T}^i_s(x) \frac{\nabla^2_x\mathcal{S}^i_s(x)}{[\mathcal{S}^i_s(x)]^2})(\xi\nabla_x\mathcal{T}^i_s(x) log\mathcal{S}^i_s(x)) (\xi^2 \nabla_x\mathcal{T}^i_s(x) \frac{\nabla_x\mathcal{S}^i_s(x)}{\mathcal{S}^i_s(x)}) (-\frac{\xi^3}{2}\nabla_x\mathcal{T}^i_s(x) \frac{\nabla^2_x\mathcal{S}^i_s(x)}{[\mathcal{S}^i_s(x)]^2}))]] \\
= -\mathbb{E}_{\xi}[\sum^k_{i=1}(\mathcal{T}^i_s(x) log\mathcal{S}^i_s(x)) ] - \mathbb{E}_{\xi}[\xi \sum^k_{i=1}(\mathcal{T}^i_s(x) \frac{\nabla_x\mathcal{S}^i_s(x)}{\mathcal{S}^i_s(x)}) ] + \mathbb{E}_{\xi}[\frac{\xi^2}{2} \sum^k_{i=1}(\mathcal{T}^i_s(x) \frac{\nabla^2_x\mathcal{S}^i_s(x)}{[\mathcal{S}^i_s(x)]^2})] \\ - \mathbb{E}_{\xi}[\xi\sum^k_{i=1}(\nabla_x\mathcal{T}^i_s(x) log\mathcal{S}^i_s(x))] - \mathbb{E}_{\xi}[\xi^2\sum^k_{i=1}( \nabla_x\mathcal{T}^i_s(x) \frac{\nabla_x\mathcal{S}^i_s(x)}{\mathcal{S}^i_s(x)})] + \mathbb{E}_{\xi}[\frac{\xi^3}{2}\sum^k_{i=1}(\nabla_x\mathcal{T}^i_s(x) \frac{\nabla^2_x\mathcal{S}^i_s(x)}{[\mathcal{S}^i_s(x)]^2}))] \\
= -\sum_{i=1}^k\mathcal{T}^i_s(x)log(\mathcal{S}^i_s(x)) - \sigma^2\sum^k_{i=1}\frac{\nabla_x\mathcal{T}^i_s(x)^T \nabla_x\mathcal{S}^i_s(x)}{\mathcal{S}^i_s(x)}
$$

$\xi = \sigma z$，z为标准正态分布，因此有:

$$
\mathbb{E}_{\xi}[\xi^p]=
\begin{cases}
0,\ p\ is\ odd\\
\sigma^p,\ p\ is\ even
\end{cases}
$$

原式：

$$
= -\mathbb{E}_{\xi}[\sum^k_{i=1}(\mathcal{T}^i_s(x) log\mathcal{S}^i_s(x)) ]+ \mathbb{E}_{\xi}[\frac{\xi^2}{2} \sum^k_{i=1}(\mathcal{T}^i_s(x) \frac{\nabla^2_x\mathcal{S}^i_s(x)}{[\mathcal{S}^i_s(x)]^2} )] - \mathbb{E}_{\xi}[\xi^2\sum^k_{i=1}( \nabla_x\mathcal{T}^i_s(x) \frac{\nabla_x\mathcal{S}^i_s(x)}{\mathcal{S}^i_s(x)})]\\
= -\sum_{i=1}^k\mathcal{T}^i_s(x)log(\mathcal{S}^i_s(x)) - \sigma^2\sum^k_{i=1}\frac{\nabla_x\mathcal{T}^i_s(x)^T \nabla_x\mathcal{S}^i_s(x)}{\mathcal{S}^i_s(x)}
$$
最后这里为了方便起见我们忽略掉超线性误差，得证。



### Proposition 2

考虑训练一个具有k个标签神经网络的平方误差代价函数$\mathcal{l}(y(x), \mathcal{S}(x))=\sum^k_{i=1}(y^i(x)-\mathcal{S}^i(x))^2$，$x\in\mathbb{R}^D$是输入数据，$y^i(x)$是类别i的标签，令$\xi(\in \mathbb{R}^D) = \sigma z$为缩放系数为$\sigma\in\mathbb{R}$的标准正态分布随机变量，因此有以下成立：

$$
\mathbb{E}_{\xi}[\sum^k_{i=1}(y^i(x) - \mathcal{S}^i(x+\xi))^2] = \sum_{i=1}^k(y^i(x)-\mathcal{S}^i(x))^2 + \sigma^2\sum^k_{i=1}||\nabla_x\mathcal{S}^i(x)||^2_2 + \mathcal{O}(\sigma^4)
$$
这正是标准的样本损失+正则化项的形式，同样结果表明以一个更适合的扩展是对于雅可比范数的惩罚而不是直接对权重进行惩罚。

对于交叉熵的情况：
$$
\mathbb{E}_{\xi}[-\sum^k_{i=1}y^i(x)log(\mathcal{S}^i_s(x+\xi))] \approx - \sum_{i=1}^ky^i(x)log(\mathcal{S}^i_s(x)) + \sigma^2\sum^k_{i=1}y^i(x)\frac{||\nabla_x\mathcal{S}^i_s(x)||^2_2}{\mathcal{S}^i_s(x)^2}
$$

#### Proofs 2.

对$\mathcal{S}(x+\xi)$做一阶泰勒展开，代入有：
$$
\mathbb{E}_{\xi}[\sum^k_{i=1}(y^i(x) - \mathcal{S}^i(x+\xi))^2] = \mathbb{E}_{\xi}[\sum^k_{i=1}(y^i(x) - \mathcal{S}^i(x) - \xi\nabla_x\mathcal{S}^i(x) )^2] + \mathcal{O}(\sigma^4) \\
= \mathbb{E}_{\xi}[\sum^k_{i=1}(y^i(x) - \mathcal{S}^i(x))^2 + \xi^2 \sum^k_{i=1} \nabla_x^2\mathcal{S}^i(x) - 2\xi\sum^k_{i=1}(y^i(x)-\mathcal{S}^i(x)\nabla_x\mathcal{S}^i(x))] + \mathcal{O}(\sigma^4) \\
= \mathbb{E}_{\xi}[\sum^k_{i=1}(y^i(x) - \mathcal{S}^i(x))^2] + \mathbb{E}_{\xi}[\xi^2 \sum^k_{i=1} \nabla_x^2\mathcal{S}^i(x)]- \mathbb{E}_{\xi}[2\xi\sum^k_{i=1}(y^i(x)-\mathcal{S}^i(x)\nabla_x\mathcal{S}^i(x))] + \mathcal{O}(\sigma^4) \\
= \sum_{i=1}^k(y^i(x)-\mathcal{S}^i(x))^2 + \sigma^2\sum^k_{i=1}||\nabla_x\mathcal{S}^i(x)||^2_2 + \mathcal{O}(\sigma^4)
$$

$\xi = \sigma z$，z为标准正态分布，因此有$\mathbb{E}_{\xi}[\xi]=0,\mathbb{E}_{\xi}[\xi^2]=\sigma^2$.

对交叉熵损失，$\mathcal{S}(x+\xi)$做一阶泰勒展开，代入有：
$$
\mathbb{E}_{\xi}[-\sum^k_{i=1}y^i(x)log(\mathcal{S}^i_s(x+\xi))] \approx 
\mathbb{E}_{\xi}[-\sum^k_{i=1}y^i(x) (log(\mathcal{S}^i_s(x) + \xi\nabla_x\mathcal{S}^i_s(x))]
$$
对$log(\mathcal{S}^i_s(x)+\xi\nabla_x\mathcal{S}^i_s(x))$做二阶泰勒展开有：
$$
log(\mathcal{S}^i_s(x)+\xi\nabla_x\mathcal{S}^i_s(x)) \approx log[\mathcal{S}^i_s(x)] + \frac{\xi\nabla_x\mathcal{S}^i_s(x)}{\mathcal{S}^i_s(x)} - \frac{1}{2}\frac{\xi^2\nabla^2_x\mathcal{S}^i_s(x)}{[\mathcal{S}^i_s(x)]^2}
$$
代入有：

$$
\mathbb{E}_{\xi}[-\sum^k_{i=1}y^i(x) (log(\mathcal{S}^i_s(x) + \xi\nabla_x\mathcal{S}^i_s(x)))] \approx \mathbb{E}_{\xi}[-\sum^k_{i=1}y^i(x) (log(\mathcal{S}^i_s(x)) +\xi\frac{\nabla_x\mathcal{S}^i_s(x)}{\mathcal{S}^i_s(x)} -\frac{\xi^2}{2}\frac{\nabla_x^2\mathcal{S}^i_s(x)}{[\mathcal{S}^i_s(x)]^2})]\\
= -\mathbb{E}_{\xi}[\sum^k_{i=1}y^i(x)log(\mathcal{S}^i_s(x))]-\mathbb{E}_{\xi}[\xi \sum^k_{i=1}y^i(x)\frac{\nabla_x\mathcal{S}^i_s(x)}{\mathcal{S}^i_s(x)}]+\mathbb{E}_{\xi}[\frac{\xi^2}{2} \sum^k_{i=1}y^i(x)\frac{\nabla_x^2\mathcal{S}^i_s(x)}{[\mathcal{S}^i_s(x)]^2}] \\
= - \sum_{i=1}^ky^i(x)log(\mathcal{S}^i_s(x)) + \sigma^2\sum^k_{i=1}y^i(x)\frac{||\nabla_x\mathcal{S}^i_s(x)||^2_2}{\mathcal{S}^i_s(x)^2}
$$

$\xi = \sigma z$，z为标准正态分布，因此有$\mathbb{E}_{\xi}[\xi]=0,\mathbb{E}_{\xi}[\xi^2]=\sigma^2$，$\sigma\in \mathbb{R}$为系数。证毕

