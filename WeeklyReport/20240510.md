# 20240510

---

## 论文阅读  

### From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels 2023ICCV

### $V_kD$：Improving Knowledge Distillation using Orthogonal Projections 2024CVPR

### Feature Normalized Knowledge Distillation for Image Classification 2020ECCV 

### Supervised Masked Knowledge Distillation for Few-Shot Transformers 2023CVPR
---

## 代码实践

#### 2024CVPR SDD
这项工作对logit进行解耦，计算图像不同尺度的logits  
**KeyPoint1：** 图像不同尺度的logits  
对模型进行修改，在前向传播时计算并保存不同尺度池化的特征图备用  
**KeyPoint2：** 对全局和局部logit预测进行加权
分为全局正确局部正确、全局正确局部错误、全局错误局部正确和全局错误局部错误四类，使用gt标间计算其标签mask，这里的实现是根据其预测的值构造了四个mask矩阵，筛选出每一类施加不同的权重来计算最终损失。

---

## 项目进展

---
