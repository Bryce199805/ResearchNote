# 20240510

---

## 论文阅读  

### From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels 2023ICCV

### $V_kD$：Improving Knowledge Distillation using Orthogonal Projections 2024CVPR

### Feature Normalized Knowledge Distillation for Image Classification 2020ECCV 

### Supervised Masked Knowledge Distillation for Few-Shot Transformers 2023CVPR

### Cumulative Spatial Knowledge Distillation for Vision Transformers 2023ICCV 

### Masked AutoEncoders Are Stronger Knowledge Distillers 2023ICCV

---

## 代码实践

#### 2024CVPR SDD
这项工作对logit进行解耦，计算图像不同尺度的logits  
**KeyPoint1：** 图像不同尺度的logits  
对模型进行修改，在前向传播时计算并保存不同尺度池化的特征图备用  
**KeyPoint2：** 对全局和局部logit预测进行加权
分为全局正确局部正确、全局正确局部错误、全局错误局部正确和全局错误局部错误四类，使用gt标间计算其标签mask，这里的实现是根据其预测的值构造了四个mask矩阵，筛选出每一类施加不同的权重来计算最终损失。

#### 2023ICCV CSKD
这项工作利用CNN辅助教学ViT训练，主要创新的包含在提出的CSKD损失类中  
**KeyPoint1:** CSKD模块，对卷积输出做全局平均池化来对齐ViT的全局特征  
在实现上简单的对教师模型的输出的h w维度求平均即可得到平均池化的结果,利用全局特征指导vit的logit进行学习  
**KeyPoint2:** CKF模块，对教师标签融合  
在训练后期削弱教师局部特征的权重，给出了线性，平方和余弦三种退化函数，对全局和局部教师特征进行融合获得融合特征来指导学生预测损失学习

---

## 项目进展

- 完成了cae项目C++接口的实现

---
