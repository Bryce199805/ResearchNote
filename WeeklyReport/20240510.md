# 20240510

---

## 论文阅读  

### From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels 2023ICCV
这项工作针对logit的耦合问题进一步进行研究，先前工作*2022CVPR DKD*虽然有效，但发现非目标类别师生模型的logits和不相同，这阻碍了其分布相似。这项工作对其进行了归一化，并且将其扩展到自蒸馏中，结合齐夫定律标签平滑给出了一套软标签的生成方案，将提出的NKD迁移到自蒸馏任务中。

### $V_kD$：Improving Knowledge Distillation using Orthogonal Projections 2024CVPR
这项工作专注于高效的转移知识，提出一个正交投影层使其能够最大限度的将蒸馏知识传递给学生模型，并且针对生成性的工作添加的辅助损失，这项工作认为这些辅助损失会损害蒸馏的性能，提出对教师模型特征进行白化来为这些生成性的任务增加多样性特征，从而避免引入额外损失来损害KD的性能。

### Feature Normalized Knowledge Distillation for Image Classification 2020ECCV
这项工作从one-hot标签角度研究了标签分布和真实分布之间的差距，作者认为one-hot将所有类别看作是相互独立并不正确，提出将真实分布表示为one-hot分布加一个噪声的形式，并通过均匀的LSD和带偏移的LSD验证了这一想法。这项工作还研究了倒数第二层对于logit蒸馏的影响，认为倒数第二层特征的L2范数表达的是标签中的噪声强度，提出一个归一化KD，使用倒数第二层特征L2范数来代替温度系数来进行蒸馏。 

### Supervised Masked Knowledge Distillation for Few-Shot Transformers 2023CVPR
这项工作致力于解决ViT小样本的设定下容易过拟合且缺乏类似归纳偏差的机制而遭受的严重性能下降的问题，这项工作提出了一种用于小样本Transformer的监督掩码知识蒸馏模型SMKD，将标签信息整合到自蒸馏框架中。首先利用师生cls的交叉熵损失和MIM损失进行自监督训练，然后引入掩码图像输入到学生模型中，利用教师模型的cls进行蒸馏，并允许跨类内图像patch之间进行蒸馏，通过余弦相似度找到师生模型类内不同图像相似patch进行蒸馏学习，最终在CIFAR-FS FC100上达到SOTA

### Cumulative Spatial Knowledge Distillation for Vision Transformers 2023ICCV 
这项工作针对ViT的蒸馏，针对使用CNN教授ViT带来的负面问题进行处理，提出了累计空间知识蒸馏CSKD，将CNN最后一个卷积层经过全局池化和全连接层获得全局特征，直接连接全连接层获得局部特征，利用全局知识向ViT中蒸馏知识，并提出累计知识融合CKF来让蒸馏前期侧重于局部知识而在后期侧重于全局知识，从而克服了先前CNN到ViT蒸馏的负面影响。

### Masked AutoEncoders Are Stronger Knowledge Distillers 2023ICCV
这项工作认为基于特征的KD方法并没有从教师中学习到完整的知识，重要的知识被简单的样本所压制导致学生很难学习到有效的知识，因此引入掩码图像建模机制来缓解这一问题，在学生图像上随机屏蔽一些块并强制其学习教师的输出来恢复缺失的特征，提出自适应解码器，包括空间对齐模块SAM、Transformers的解码器和空间恢复模块SRM，通过该模块对学生模型FPN提取的带掩码的特征图与教师模型的特征图对齐进行蒸馏学习，并且在总损失中引入了2022CVPR的一项全局损失结构GCBlock来辅助蒸馏学习。

### Masked AutoEncoders Enable Efficient Knowledge Distillers 2023CVPR 
这项工作研究了从预训练模型MAE中提取知识的能力，与先前工作不同，提出对师生模型中间特征表示进行匹配，这使得繁琐的教师模型只需要前几个层进行前向传播，节约了计算开销，通过一个投射层对齐师生模型特征进行匹配，使用L1范数距离来确保中间特征对齐。并且将MAE的掩码率提高到95%进一步减少计算。最终的蒸馏范式符合标签损失+蒸馏损失的形式。

---

## 代码实践

最近有些想法，准备做一些实验收集一些数据，目前还在搭建架构没什么成果。

#### 2024CVPR SDD
这项工作对logit进行解耦，计算图像不同尺度的logits  
**KeyPoint1：** 图像不同尺度的logits  
对模型进行修改，在前向传播时计算并保存不同尺度池化的特征图备用  
**KeyPoint2：** 对全局和局部logit预测进行加权
分为全局正确局部正确、全局正确局部错误、全局错误局部正确和全局错误局部错误四类，使用gt标间计算其标签mask，这里的实现是根据其预测的值构造了四个mask矩阵，筛选出每一类施加不同的权重来计算最终损失。

#### 2023ICCV CSKD
这项工作利用CNN辅助教学ViT训练，主要创新的包含在提出的CSKD损失类中  
**KeyPoint1:** CSKD模块，对卷积输出做全局平均池化来对齐ViT的全局特征  
在实现上简单的对教师模型的输出的h w维度求平均即可得到平均池化的结果,利用全局特征指导vit的logit进行学习  
**KeyPoint2:** CKF模块，对教师标签融合  
在训练后期削弱教师局部特征的权重，给出了线性，平方和余弦三种退化函数，对全局和局部教师特征进行融合获得融合特征来指导学生预测损失学习

---

## 项目进展

- 完成了cae项目C++接口的实现
- 研究了hdf5对于文件的存储，核心思想是二进制流的控制

---

## Mathematics

学习了图像的频率域的处理，主要集中在研究傅里叶变换和小波变换  
refs: 数字图像处理 --冈萨雷斯 ch4 频率域滤波  ch7 小波变换和其他图像变换
