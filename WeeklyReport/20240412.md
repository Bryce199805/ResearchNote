# 20240412

---

这周上课结课汇报，浪费了不少时间，总体工作量较之前较少

## 论文阅读

> Features
### Cross-Layer Distillation with Semantic Calibration 2021AAAI
这项工作提出了一种新的迁移中间层特征的方法，先前工作特征图之间的简单匹配可能会导致语义不匹配，从而出现学生模型中的负面正则化效应，这项工作基于此出发提出SemCKD，在中间层利用相似度矩阵和注意力机制，利用注意力机制计算学生特征图于教师特征图之间的相似程度，有效的将学生层与教师模型中语义相似的层进行绑定，有效抑制了负正则化效应，达到了更好的性能。

> 这两项工作的核心思想都是对于教师模型分类器的重用，两者的区别是后者对于师生模型的输出也不做损失只对分类器的前一层的特征图做一个L2损失
### Knowledge Distillation via Softmax Regression Representation Learning 2021ICLR
这项工作提出了两个损失函数，第一个直接用来匹配特征，专注于倒数第二层的特征，类似于FitNets；第二个损失计算logits，师生模型都利用教师的分类器计算一个输出，在这里优化一个分类损失来帮助学生模型对教师模型特征层的学习。

这项工作与2022CVPR-SimKD很相似，后者核心观点也是重用教师模型的分类器

### Knowledge Distillation with the Reused Teacher Classifier 2022CVPR
先前的工作致力于精心设计的知识表示。这项工作提出教师模型的强大性能不仅归功于前面的特征表达，最后的判别器也同样重要，基于这一假设，这项工作通过一个投射层简单对齐师生模型的特征，通过一个L2损失让学生特征去模仿教师的特征，直接使用教师模型的分类器进行学生模型的推理。

> 从互信息的角度给出了不同于之前工作的损失，针对贝叶斯变分法，整理了相关知识
### Variational Information Distillation for Knowledge Transfer 2019CVPR
这项工作从信息论的角度试着解释知识转移的过程，提出了变分信息蒸馏，将知识转移表述为教师网络和学生网络之间的互信息的最大化，提出一个可操作的知识转移目标，允许我们量化从教师网络到学生网络的知识量，并且通过最大化变分下界来近似分布来帮助互信息的计算。

---

> 对教师模型的修改与观察
### Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students 2019AAAI
这项工作认为教师模型使用硬标签训练过于严格，次要类中也包含这许多信息，应该训练一个更宽容的教师来教授学生，这项工作提出要考虑次要类别的影响，视觉上相似的物体同样能够提供有用的信息，在训练教师模型的损失中添加一项，计算主类与其他k-1个类别之间的置信度差异，通过这种手段来加强次要类别的置信度从而训练出一个更宽容的教师。

### Revisiting Knowledge Distillation via Label Smoothing Regularization 2020CVPR
这项工作通过实验证明了用一个较弱的学生去教授老师，和使用一个训练不佳的老师去教授一个比他性能更好的学生，都会带来其性能的提升，这与先前的认知不相符。通过这个这项工作认为暗知识不仅包括类被之间的相似性，还包括对学生培训的正则化，然后通过标签平滑正则化LSR的角度重新审视的KD，提出了一个自训练框架Tf-KD

---

> Applications
### Class-relation Knowledge Distillation for Novel Class Discovery 2023ICCV (code)
本文提出了一种利用类关系蒸馏的增量学习方法，将新类发现任务转化为一个最优传输问题利用Sinkhorn算法进行求解，为解决灾难性以往问题，使模型能够记住先前的类别，提出了类别关系蒸馏，将新类发现阶段的就类别输出头的预测输出根据权重进行标准化后，与预训练的监督模型进行蒸馏，让两者的分布尽可能的相似。

### A Fast Knowledge Distillation Framework for Visual Recognition 2022ECCV
这项工作针对传统蒸馏框架与普通模型训练相比多了教师模型推理的开销，先前工作针对这一问题的处理缺乏一致性，并且繁琐的后处理过程也引入了不必要的开销，这项工作针对保存全局标签图因随机数据增强带来的不一致性进行了修正，提出在教师模型进行推理的阶段保存数据增强以及输出结果等信息，在训练学生模型时采用相同的增强策略，并复用其输出结果进行训练。

### FerKD: Surgical Label Adaptation for Efficient Distillation 2023ICCV
这项工作针对传统蒸馏模型中对学生模型进行训练时对输入样本在教师模型上进行计算软标签这一过程引入的额外开销进行了削减，先前的方法FKD提前根据数据增强方法生成一致性区域标签，但是在随机裁剪生成区域时可能包括背景和一些不相关的噪声以及上下文信息等，为了处理这个问题这项工作提出自适应的标签校正，将样本分为四类，丢弃掉非常简单和非常困难的样本，并重点优化另外两种样本；并且还发现非常强的数据增强可能会损害模型性能，我们针对软标签提出了一种新的selfMix增强方法，降低数据增强的强度，同时仍然受益于其效果。

---

## 代码实践
复现了上述标记code的代码和上周的两篇较新的文章的代码

#### Multi-level Logit Distillation 2023CVPR
这项工作提出了多级logit蒸馏，主要是针对其损失函数的设计进行了探究，代码中利用了不同的温度系数的聚合和强弱数据增强对特征进行了增强

#### DisWOT: Student Architecture Search for Distillation Without Training 2023CVPR
这项工作的代码比较繁琐又没有清楚的说明文件，现在是一个很混乱的状态。研究代码发现文章中所说的Evolutionary Search并不是指进化算法，还是使用的NAS搜索的架构。整体的训练流程是分开的，其所说的Without Training指的是在蒸馏阶段不需要进行模型搜索的训练，在预处理步骤中会利用文中提出的评价指标搜索出相应数据集下教师模型最适合学生模型的结构。

#### 2023CR-KD-NCD
这是一项增量学习的工作，主要研究了其模型架构，以及其设计的复杂的损失函数，其中包括一个Sinkhorn求解最优传输问题。


---

## 项目进展
接手核学院的项目，学习go语言和fiber框架

---

## Mathematics

在变分贝叶斯方法中，变分下界(Evidence Lower Bound, ELBO)是一种用于估计一些观测数据的对数似然下界。

### Definition

设X和Z为随机变量，其联合分布为$p_\theta$，例如$p_\theta(X)$是X的边缘分布，$p_\theta(Z|X)$为在给定X的条件下Z的条件分布，则对任何从$p_\theta$中抽取的样本$x \sim p_\theta$和任何分布$q_\phi$，有：
$$
log\ p_\theta(x) \geq \mathbb{E}_{z\sim q_\phi}[ln\frac{p_\theta(x, z)}{q_\phi(z)}]
$$
上式称为ELBO不等式，左侧为x的证据，右侧为证据下界。

### Motivation

假设我们有一个可观察的随机变量X，且我们想找其真实分布$p^*$，这将允许我们抽样生成数据来估计未来事件概率。但是精确找到$p^*$是不可能的，需要寻找一个近似。定义一个足够大的参数化分布族$\{ p_\theta \}_{\theta\in \Theta}$， 最小化某种损失函数L，$\underset{\theta}{min\ L(p_\theta, p^*)}$。解决该问题的一种方法是考虑从$p_{\theta}$到$p_{\theta+\delta\theta}$的微小变化，并使得$L(p_\theta, p^*) - L(p_{\theta+\delta\theta}, p^*) = 0$，这是变分法中的一个变分问题。

我们考虑隐式参数化的概率分布：

- 定义一个在潜在随机变量Z上的简单分布p(z) （高斯分布、均匀分布等）
- 定义一个由$\theta$参数化的复杂函数族$f_\theta$（神经网络）
- 定义一种将任何$f_\theta(z)$转化为可观测随机变量X的简单分布的方法，如$f_{\theta}(z) = (f_1(z), f_2(z))$，可以将相应分布X定义在高斯分布上$N(f_1(z), e^{f_2(x)})$

这构造了一个关于（X， Z）的联合分布族$p_\theta$， 从$p_\theta$中抽取样本$(x, z) \sim p_\theta$非常容易，秩序从p中抽样z~p, 然后计算$p_\theta(z)$，最后使用$f_\theta(z)$来抽样$x\sim p_\theta(·|z)$。我们拥有了一个可观测和潜在随机变量的生成模型。

我们想要构造一个$p^*$使得$p^*(X) \approx p_\theta(X)$， 右侧需要对Z进行边缘化来消除Z的影响。我们无法计算$p_\theta(x) = \int p_\theta(x|z)p(z)dz$需要寻找一个近似。根据贝叶斯公式：
$$
p_\theta(x) = \frac{p_\theta(x|z)p(z)}{p_\theta(z|x)}
$$
我们需要找到一个$q_\phi(z)$来近似$p_\theta(z|x)$，这是一个针对潜变量的判别模型

###  Deriving the ELBO

引入一个新的分布$q_\phi(z)$作为潜变量z的后验分布$p_\theta(z|x)$的近似，边际对数似然$log\ p_\theta(x)$可以表示为：  
$$  
\begin{aligned}
log\ p_\theta(x) &= log[\frac{p_\theta(x, z)}{p_\theta(z|x)}] = log\ p_\theta(x, z) - log\ p_\theta(z|x) \\
&= log\ p_\theta(x, z) -log\ q_\phi(z)  - log\ p_\theta(z|x) + log\ q_\phi(z) \\
&= log[\frac{p_\theta(x, z)}{q_\phi(z)}] + log[\frac{q_\phi(z)}{p_\theta(z|x)}]
\end{aligned}
$$

两边求$q_{\phi}(z)$的期望：

$$
\begin{aligned}
&\mathbb{E}_{z\sim q_\phi(z)}[log\ p_\theta(x)] = \sum_z q_\phi(z) log\ p_\theta(x) = log\ p_\theta(x)\sum_zq_\phi(z) = log\ p_\theta(x) \\
&\mathbb{E}_{z\sim q_\phi(z)}[log[\frac{p_\theta(x, z)}{q_\phi(z)}] + log[\frac{q_\phi(z)}{p_\theta(z|x)}]] = \mathbb{E}_{z\sim q_\phi(z)}[log[\frac{p_\theta(x, z)}{q_\phi(z)}]] + \mathbb{E}_{z\sim q_\phi(z)}[log[\frac{q_\phi(z)}{p_\theta(z|x)}]] \\
&=\sum_z q_\phi(z) log[\frac{p_\theta(x, z)}{q_\phi(z)}] + \sum_zq_\phi(z)log[\frac{q_\phi(z)}{p_\theta(z|x)}] = \mathcal{L} + KL(q_\phi(z) || p_\theta(z|x)) \\
\end{aligned}
$$
因此有：
$$
log\ p_\theta(x) = \mathcal{L} + KL(q_\phi(z) || p_\theta(z|x)) \\
\mathcal{L}_{ELBO} = \sum_z q_\phi(z) log[\frac{p_\theta(x, z)}{q_\phi(z)}] = \mathbb{E}_{z\sim q_\phi(z)}[log[\frac{p_\theta(x, z)}{q_\phi(z)}] = log\ p_\theta(x) - KL(q_\phi(z) || p_\theta(z|x))
$$
由此推出ELBO变分下界，其中KL散度是非负的，$\mathcal{L}_{ELBO} \leq log\ p_\theta(x)$，因此可以通过最大化ELBO来近似最大化边际对数似然。
