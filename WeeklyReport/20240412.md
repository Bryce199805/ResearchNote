# 20240412

---

## 论文阅读
> Features
### Cross-Layer Distillation with Semantic Calibration 2021AAAI

### Knowledge Distillation via Softmax Regression Representation Learning 2021ICLR

### Knowledge Distillation with the Reused Teacher Classifier

---

### Revisiting Knowledge Distillation via Label Smoothing Regularization 2020CVPR
这项工作通过实验证明了用一个较弱的学生去教授老师，和使用一个训练不佳的老师去教授一个比他性能更好的学生，都会带来其性能的提升，这与先前的认知不相符。通过这个这项工作认为暗知识不仅包括类被之间的相似性，还包括对学生培训的正则化，然后通过标签平滑正则化LSR的角度重新审视的KD，提出了一个自训练框架Tf-KD

---

### Class-relation Knowledge Distillation for Novel Class Discovery
---

## 代码实践

---

## 项目进展

---

## Mathematics
