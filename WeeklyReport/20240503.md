# 20240503

---

## 论文阅读  
目前对于蒸馏研究的趋势正在靠logit蒸馏和蒸馏的优化靠拢，主要集中在对温度系数的研究和对蒸馏中复杂关系的解耦。  
最新的CVPR2024有项工作第一次将频率域的知识引入到了蒸馏学习中。

---

> KAN  MLP的替代品！
> 阅读源码发现，KAN现在无法像卷积一样处理图像相关的视觉任务，只能将图像拉直后处理，近期我认为会出现大量基于此的工作类似于ConvKan的出现
### KAN: Kolmogorov–Arnold Networks 2024arXiv

麻省理工学院与加州理工学院最新成果，4.30上传到arXiv目前GitHub源码已获赞2.7k。

提出了一种MLP的替代网络，更少的参数，更高的精度，更好的可解释性。MLP的理论支持是通用近似定理，KAN则是发展于柯尔莫哥洛夫-阿诺德表示定理，其主要思想是将MLP中固定的在节点上的激活函数转移到可学习的边上的激活函数。 

柯尔莫哥洛夫-阿诺德表示定理： 如果f是一个定义在有界域上的多变量连续函数，那么该函数就可以表示为多个单变量，加法连续函数的有限组合。 

原始的KA表示定理对应于两层的KAN网络，参考MLP的扩展结构将其拓展到多层KAN网络，每个单变量函数由B样条曲线来刻画，其系数是可学习的，在优化学习过程中设置一个网格区间，样条曲线可以任意精确到目标函数，从粗网格逐渐细化到细网格来获得更精细的B样条曲线，使用拟牛顿法BFGS进行优化，损失以阶梯状下降，学习是从全连接的KAN开始。实验发现对不同的输入数据KAN的最优架构是不同的，提出一种通过正则化和剪枝的方法来搜索这种架构，给定顶点入度出度的评价指标对网络进行剪枝。

文章介绍了大量实验来说明其性能、精度、可解释性，并且在一些数学物理问题上求解进行了实验对比(这部分看不太懂)。

---

> 利用小波变换第一次将频率域的知识应用到知识蒸馏中，计划复习一下之前学的图像频率知识，目前是用于做密集预测任务，我感觉这一表示学习方法可以推广到更普适的任务中；且这项工作的全局表示学习比较牵强我认为可以进一步优化
### FreeKD: Knowledge Distillation via Semantic Frequency Prompt 2024CVPR  
首次提出从频率域对特征进行蒸馏学习，对教师模型提出使用提示Prompt模块并结合Jaccard相似性系数来获得掩码图计算频率域中的兴趣点集合，从而来提高蒸馏的效率；对学生模型提出位置相关损失来为学生模型提供更高阶的空间增强，通过小波变换转换到频率与并通过MSE损失来模仿其频率特征。

---
> 这一组都是关于温度系数的研究，近期关于这方面的研究内容在逐渐增加
### Annealing Knowledge Distillation 2021ACL
针对师生网络能力差距较大时产生的性能下降问题，在温度系数上进行了优化来缓解这一问题，提出AKD，将蒸馏分为两阶段，第一阶段用MSE代替kl散度，在教师的引导下进行学习，初期给教师模型施加一个大的温度系数使之平滑让学生更容易进行学习，并随着轮次的进行逐步降低温度使之学习教师模型更尖锐的输出；第二阶段移除教师模型，使用数据集的标签对学生模型进行微调。

### Norm KD: Normalized Logits for Knowledge Distillation 2023arXiv
这项工作针对温度系数进行研究，先前工作将其设为固定值这并不合适，针对不同的样本相同的温度并不能很好的软化所有的样本，因此提出了归一化知识蒸馏，将每个样本的logits分布视为高斯分布对齐标准化，融入到softmax函数中仅需计算每个logits输出的标准差，并且引入一个Tnorm超参数来调整分布，这种方法几乎没有引入额外的计算开销并且可以容易的与其他基于logits的方法向结合。

### Dynamic Temperature Knowledge Distillation 2024arXiv
这项工作从两个角度考虑了温度系数的影响，从模型的角度来看调节温度能缓解师生模型之间的能力差距，从任务的角度来看调节温度有助于设置合适的任务难度等级，提出了logsumexp来评价师生模型logits输出的平滑度，并提出通过调节温度系数来优化使得师生模型之间的平滑度差异最小，来缓解KD对任务带来的负优化问题。

### Logit Standardization in Knowledge Distillation 2024CVPR
这项工作研究了温度系数的作用，从信息论的角度利用最大熵理论推导softmax函数，温度系数是以拉格朗日乘子法的系数出现的，并基于此分析了传统KD对师生模型使用相同温度的弊端，提出了分别使用其各自logit均值和标准差进行标准化，来打破传统KD的弊端

---
> 这项工作是针对logit蒸馏的研究，对logit进行解耦优化蒸馏过程
### Scale Decoupled Distillation 2024CVPR 
这项工作认为现有的logits方法只利用了多种语义知识耦合的全局logits输出，这可能会将模棱两可的知识传递给学生，提出了一项多尺度的logit蒸馏方法，将全局的logits解耦为多个不同尺度的局部logits，学生模型与教师模型中对应尺度的logits进行学习，一致项将响应类别的多尺度知识传递给学生，互补项为学生保留了样本歧义。

---

## 代码实践

#### 2024CVPR Logit Standardization KD

这项工作的亮点在于得出结论的分析，代码没有什么亮点，但logit标准化这一方法可以很简单的插入到其他基于logit的蒸馏方法中，只需要在计算softmax之前对logit进行标准化即可。

#### 2022ECCV KDC
这项工作提出了知识筛选机制.
实现上的核心在于每一轮之后计算该批次样本的熵并保存下来，核心代码在于dataloader的构造，在dataloader中添加了一个update方法，根据论文中给出的公式利用熵和频率计算得分并排序，将样本分为高价值样本、低价值样本和废弃样本，高价值样本直接保留用于蒸馏，低价值样本利用废弃样本做一个扰动增强用于蒸馏，实现层面是一个按比例融合，废弃样本直接丢弃。从数据集上更新知识来实现筛选机制。

#### 2024arXiv KAN

源码已经集成到pip中可以直接安装，给出的GitHub仓库更像是一个教程，给了大量例子和教程notebook来教学如何使用KAN模块网络。
代码高度封装化，非常容易调用。
目前的KAN不能像卷积一样处理图像任务，在讨论区爆发大量讨论，我认为近期会有相关工作基于此提出ConvKAN来适应视觉领域。

---

## 项目进展
搭建了一套32位的C++开发环境，处理了之前64位机无法读取32位动态库的问题.

---
