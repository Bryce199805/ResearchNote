# 20240503

---

## 论文阅读

### FreeKD: Knowledge Distillation via Semantic Frequency Prompt 2024CVPR

### Dynamic Temperature Knowledge Distillation 2024arXiv

### Norm KD: Normalized Logits for Knowledge Distillation 2023arXiv

### Annealing Knowledge Distillation 2021ACL

### Logit Standardization in Knowledge Distillation 2024CVPR

### Scale Decoupled Distillation 2024CVPR 

---

## 代码实践

#### 2024CVPR Logit Standardization KD

这项工作的亮点在于得出结论的分析，代码没有什么亮点，但logit标准化这一方法可以很简单的插入到其他基于logit的蒸馏方法中，只需要在计算softmax之前对logit进行标准化即可。

#### 2022ECCV KDC
这项工作提出了知识筛选机制.
实现上的核心在于每一轮之后计算该批次样本的熵并保存下来，核心代码在于dataloader的构造，在dataloader中添加了一个update方法，根据论文中给出的公式利用熵和频率计算得分并排序，将样本分为高价值样本、低价值样本和废弃样本，高价值样本直接保留用于蒸馏，低价值样本利用废弃样本做一个扰动增强用于蒸馏，实现层面是一个按比例融合，废弃样本直接丢弃。从数据集上更新知识来实现筛选机制。

---

## 项目进展



---
