# 20240607

近期计划写一篇中文核心的文献综述一边读最新论文一边补充；实验上有些想法做一下实验看一下效果

---

## 论文阅读  

> 跨架构蒸馏  据我了解唯一一项从Vision Transformer 到 CNN的知识转移的工作
### Cross-Architecture Knowledge Distillation 2022ACCV
这项工作提出了一个跨架构的蒸馏方案，将Transformer的知识传递给CNN网络，提出部分交叉注意力层和分组线性层来促进师生模型特征之间的可转移性，为了使模型更加鲁棒，还提出一个多视角的鲁棒训练方案，引入多种数据增强手段来提升学生学生的鲁棒性。

- 将Transformer知识转移到CNN中，提出部分交叉注意力投射层和分组线性投射层来映射学生特征使之与教师特征更相似
- 提出一个多视角的增强方案，增加学生网络的鲁棒性

---

> ViT的蒸馏 同架构蒸馏 压缩ViT的大小，其中TinyViT MiniViT均出自微软
### TinyViT: Fast Pretraining Distillation for Small Vision Transformers 2022ECCV
Vision Transformer模型效果很好但规模太大难以部署到计算有限的平台上使用，提出一种蒸馏方法来预训练小的Transformer模型，使其能够从海量的预训练知识中获得性能的提升，由于大的教师模型直接推理占用大量资源，将教师模型推理与学生模型蒸馏分离，提出稀疏logit编码和数据增强编码存储来存储教师模型的推理结果，并通过模型收缩方法得到最优的Tiny-ViT架构模型。

- 将 ViT 的预训练知识蒸馏到较小的Tiny-ViT模型中
- 提出了稀疏logit编码和数据增强编码来增加存储效率
- 通过模型收缩方法得到最优的学生模型架构（Tiny-ViT）

### MiniViT: Compressing Vision Transformers with Weight Multiplexing 2022CVPR
这项工作研究了权重共享技术，权重共享直接利用到vision Transformer上会带来训练不稳定和性能下降的问题，提出了使用权重复用和权重蒸馏来缓解这些问题，权重复用在MHA的softmax前后插入线性层，在MLP中插入转换模块，这些参数不共享来提高参数的多样性；权重蒸馏考虑了输出的logit、自注意力的QKV矩阵和MLP的关系矩阵，通过这两个模块来缓解权重共享技术在视觉Transformer中的弊端。

- 权重共享技术利用到ViT中
- 提出权重复用：在MHA的softmax前后插入线性层，在MLP中插入转换模块
- 提出权重蒸馏：输出的logit、自注意力的QKV矩阵、MLP的关系矩阵

### ViTKD: Practical Guidelines For ViT Feature Knowledge Distillation arXiv2209
这项工作研究了ViT模型浅层和深层特征的分布特征，提出不同层需要不同的蒸馏策略，对于浅层使用基于模仿的线性特征蒸馏，对于深层特征使用基于Mask的生成式蒸馏。

- ViT不同层具有不同的特征分布，使用相同的蒸馏策略不合理
- 浅层使用基于模仿的蒸馏策略
- 深层使用基于Mask掩码的蒸馏策略

---

> 基于遗传算法搜索来解决师生差距问题
### UniADS: Universal Architecture-Distiller Search for Distillation Gap 2024AAAI
这项工作为了弥合师生模型之间差距带来的性能损失，提出了一种基于NSGA-II遗传算法的搜索框架，同时考虑优化精度和模型参数来进行搜索，并提出一种连续减半的优化加速策略对其进行剪枝，并且做了消融实验对参数重要性进行测试，最终在CIFAR ImageNet上达到SOTA。

- 使用NSGA-II 遗传算法搜索框架，同时考虑优化精度和模型参数
- 提出连续减半的优化加速策略

> 两项关于小样本的类增量学习的任务
### MIND: Multi-Task Incremental Network Distillation 2024AAAI
这项工作提出了一种新的增量学习框架，通过蒸馏技术来保存先前任务的知识，对于每一个子任务提出随机权重选择微调后融合到主干模型中，模型训练时前向计算会使用先前类的知识，优化时先前类的权重被冻结只更新当前任务的权重，通过二进制的掩码来保存每一阶段的子网权重位置。为了适应内存受限的环境将其扩展到自蒸馏模型。选择主干中最重要的权重来获得子网。最终性能达到SOTA

- 配备蒸馏机制的参数隔离增量学习方法
- 为蒸馏和自蒸馏范式提出了不同的剪枝策略获得子网
- 提出了门控机制，通过二进制掩码来保存子网权重，来引导反向传播

### M2SD: Multiple Mixing Self-Distillation for Few-Shot Class-Incremental Learning 2024AAAI
这项工作提出一种混合多重自蒸馏方法，来解决小样本增量学习的灾难性遗忘问题，其核心思想是构造一个能够识别不同类别数据的特征空间，提出构造虚拟类来训练模型，来扩大模型的特征空间范围，通过mixup和cutup来构造虚拟实例，并提出了一种基于注意力的分层特征融合方式，将通过虚拟类和注意力特征融合后的知识蒸馏给源模型。

- 提出虚拟类的方式来扩大模型特征空间
- 提出基于自注意力和坐标注意力的分层特征融合方式

---

## 论文写作

准备写一篇关于知识蒸馏的中文核心文献综述，本周研究了latex的使用并且在整理读过的文章
目前综述的结构分布大致为：  
1. 引言
2. KD的提出背景
3. KD的研究现状  
   这部分作为主体，四个层面的方法大概有一百篇文献，后续边读边更新
   1. logit蒸馏
   2. Feature蒸馏
   3. Transformer架构蒸馏
   4. 蒸馏学习的优化
4. KD的应用场景  
    这部分文献目前已经读过的较少，后续需要补充
5. 总结展望

---

## 代码实践

实验部分想要做一个跨架构的蒸馏，将Transformer的知识转移到CNN架构中，目前只发现2022ACCV的一项工作做了这个事，他们的观点是浅层特征ViT与CNN架构特征差异较大
我目前的想法：
- 先看看直接做ViT到CNN的蒸馏，效果有多差，差在哪里
- 跨架构浅层特征差异大，我想对学生CNN浅层特征做自蒸馏，在深层特征做全局池化与ViT的特征对齐
- 这件事的关键在于如何处理跨架构知识的对齐，从CNN到Transformer的工作较多，考虑借鉴他们的思想逆向使用到Transformer到CNN的结构上

---

## 项目进展

- 研究了cae后端go代码

---


