# 20240426

---

## 论文阅读

> logits 安全保护
### UnDistillable: Making A Nasty Teacher That Cannot Teach Students 2021ICLR

> logits 多教师KD
### ORC: Network Group-based Knowledge Distillation using Online Role Change 2023ICCV

> Search 蒙特卡洛
### Automated Knowledge Distillation via Monte Carlo Tree Search 2023ICCV

> logits 优化 寻找平坦最优解 OKD
### Generalization Matters: Loss Minima Flattening via Parameter Hybridization for Efficient Online Knowledge Distillation 2023CVPR

> relation 大模型蒸馏
### TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models 2023CVPR

> logits ViT蒸馏
### Train Data-Efficient Image Transformers & Distillation Through Attention 2021NeurIPS

> logits ViT蒸馏 归纳偏差
### Co-advise: Cross Inductive Bias Distillation 2022CVPR
---

## 代码实践


---

## 项目进展


---

## Mathematics


