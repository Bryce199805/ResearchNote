# 20240329

---

## 论文阅读

> 这两项工作是针对于利用dropout或数据增强策略来集成获得更好模型的方法，后者是针对前者训练慢问题的改进工作
### Temporal Ensembling for Semi-Supervised Learning 2017ICLR
本文提出了一种基于集成网络的半监督学习方法，给出了两种不同的网络结构，通过有监督和无监督相结合的方式来进行学习，无监督的信号来源于不同的dropout,数据增强策略以及不同的epoch的输出，通过最小化当前输出与先前网络的输出或不同策略的输出，来获得最好的模型

### Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results 2017NeurIPS
这项工作针对2017ICLR提出的时序集成模型做出了改进，原模型学习到的知识以很慢的速度融入到训练过程中，这导致训练速度较慢，本文通过引入学生模型的EMA权重来逐步更新教师模型的权重，从而加快训练进程。

---

> 这一组是关于Knowledge Transfer工作的内容，包括了一些上周阅读的文献
### FitNets: Hints For Thin Deep Nets 2015ICLR-0322
### Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer 2017ICLR-0322
### Paraphrasing Complex Network: Network Compression via Factor Transfer 2018NeurIPS-0322

### A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning 2017CVPR 
提出了一种新的知识转移策略，提出FSP矩阵来表示网络层中间问题求解的过程，最小化师生模型中间层的L2损失来进行知识的转移

### Knowledge Transfer with Jacobian Matching 2018PMLR
本文提出了通过雅可比矩阵的匹配来进行师生模型之间的知识转移，本文认为两个网络的雅可比矩阵匹配等价于在训练过程中对输入添加噪声的软标签进行匹配，通过泰勒公式对神经网络进行仿射逼近，验证了这一观点。 

> 这项工作涵盖对比了大部分比较热门的KT方法，并总结其优缺点提出了自己的方法，本组大部分文献都涵盖与这项工作中
### A Comprehensive Overhaul of Feature Distillation 2019ICCV (code)
这项工作详细对比了近期六项较为具有代表性的kt方法，分析其优缺点并给出本文的方法。从教师特征变换、学生特征变换、特征蒸馏位置、距离函数四个方面论述了各种方法的优缺点。本文提出在进行ReLU激活前进行特征的转移，设计了一种新的教师特征变换的模式，并在这基础上提出了一种新的距离度量。


> 这项工作也是关于KT的方法，但是转移的是教师模型的激活状态信息，与上周读的两篇为同类
### Similarity-Preserving Knowledge Distillation 2019-ICCV
这项工作提出了一种新的知识迁移方法，其灵感来自于语义相似的输入倾向于在经过训练的神经网络中引发相似的激活模式，利用小批量样本之间的相似性来将教师网络中的输出激活图迁移到学生模型上。先前的蒸馏方法鼓励学生模仿教师表征空间的不同方面，这项旨在保持输入样本的成对激活相似性，不需要能够表达教师的表征空间而只需要保留成对的激活相似性即可。
### Like What You Like: Knowledge Distill via Neuron Selectivity Transfer 2017arXiv-0322
### Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons 2019AAAI-0322

---

> 这一组同样为关于KT的方法，但是他不光考虑了样本的知识，还考虑了样本间关系以及结构关系
### DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer 2018AAAI-0322
### Corrlation Congruence for Knowledge Distillation 2019ICCV
先前工作仅考虑样本实例之间的一致性关系而忽视了相关性的关系，直接模仿教师模型的输出使得学生模型并不能很好的呈现类内聚集而类间分离的特征，这项工作提出一个新的蒸馏框架CCKD，通过蒸馏来保持样本一致性，并引入相关性一致，通过核方法捕获样本之间的相关性，将其融入蒸馏学习的过程中，并且就计算相关性给出了两种类别采样方法。

### Knowledge Distillation via Instance Relationship Graph 2019CVPR
先前工作忽略了实例之间的关系，本文提出一种实例关系图用于知识蒸馏，建模了三种知识：实例特征、实例关系和特征空间变换。将实例特征和实例关系分别作为顶点和边，引入三种损失分别对其进行优化。

### Relational Knowledge Distillation 2019CVPR (code)
先前方法令学生模仿教师对于单一示例输出而忽略了样本之间的关系，我们提出称为关系知识蒸馏RKD的方法，转移了数据示例之间的相互关系，提出了距离和角度的蒸馏损失，来惩罚师生模型之间的结构差异。

---

>  *A Comprehensive Overhaul of Feature Distillation 2019ICCV*这项工作其中提到了BN层的训练模式与推理模式的不同，不是很明白，追溯了一下这一观点的源头
### Batch Renormalization: Towards Reducing Mini-Batch Dependence in Batch-Normalized Models 2017NeurIPS
批量化虽然有效，但不太适合小型或非独立同分布的minibatch，我们认为是由于模型中的这些激活在训练和推理阶段计算方式不同导致的。这项工作改进了BatchNorm层，不在直接使用小批量内数据的均值方差来进行归一化操作，而是引入一个通过小批量统计量累计计算的总体统计量，来纠正小批量与总体统计量之间的差异。

--- 

> 这两项工作介绍的是具体的应用场景，一项是半监督目标检测，一项是语义分割，均提出了自适应蒸馏的思想
### Learning Efficient Detector with Semi-supervised Adaptive Distillation 2019arXiv
本文针对半监督目标检测中难以学习的样本和难以模仿的样本提出了自适应的蒸馏损失，并提出一种简单的过滤机制来筛选未标记的数据，从而使之能够高效的向学生模型传递知识。

### Knowledge Adaptation for Efficient Semantic Segmentation 2019CVPR
这项工作将教师网络中的知识转化为更具信息含量的压缩空间，训练了一个自编码器来挖掘隐含的结构信息将知识翻译成更容易被学生模型理解的格式（这里的思想与*Paraphrasing Complex Network: Network Compression via Factor Transfer 2018NeurIPS*非常相似）；由于师生模型之间的差异，小模型由于有限的感受野和抽象能力很难捕获教师网络中的长程依赖关系，提出一个亲和蒸馏模块来学习这种长程依赖。

---

## 代码实践
复现运行了上述标记code的文献，着重研究了对于**复杂的损失函数以及训练过程**如何编码实现

---

## 项目进展
研究了postman工具，对Kubesphere Api进行了粗分类，做了一些请求测试

---

## Mathematics
近期文献阅读发现多篇文献使用了核方法，研究整理了一下相关知识
> 关于一些常用的核函数推导还未整理完  
> 有文献用到了核函数的p阶泰勒展开做近似计算，这个地方还没有推导明白

# Kernel Method

**核方法**是指通过**核技巧**把样本输入空间的非线性问题转换为特征空间的线性问题，通过引入**核函数**不显式地定义特征空间和映射函数，而是将优化问题的解表示成核函数的**线性组合**。

对于一个线性模型$f(x)=w^Tx$忽略偏置项b，根据表示定理，在满足一定条件时，参数w的最优解可以表示为N个样本数据的线性组合$w^*=\sum^N_{i=1}\alpha_ix_i$，因此最优解表示为：
$$
f^*(x)=\sum^N_{i=1}\alpha_ix_i^Tx
$$
引入映射$\phi:\mathcal{X} \rightarrow \mathcal{F}$，将样本空间$\mathcal{X}$变换到高维的特征空间$\mathcal{F}$，从而将在样本空间中线性不可分的样本在特征空间中线性可分（**若原始样本空间为有限维，则一定存在一个高维的线性空间使得样本线性可分**），为模型增加非线性的表示能力，则对应模型最优解为：
$$
f^*(x)=\sum^N_{i=1}\alpha_i\phi(x_i)^T\phi(x)
$$
直接计算特征变换$\phi(x)$以及特征变换的内积$\phi(x)^T\phi(x')$相当困难，这可能会把样本投射到无穷维度的特征空间，最优解中仅出现了特征变换的内积，因此引入核函数$k(x, x')=\phi(x)^T\phi(x')$，将计算特征变换的内积$\phi(x)^T\phi(x')$转化为计算核函数的值$k(x, x')$。

- **核技巧(Kernel Trick)**是指通过一个非线性变换$\phi$把输入空间(如低维的欧式空间)映射到特征空间(如高维的希尔伯特空间)，从而把输入空间的非线性问题转换为特征空间的线性问题
- **核函数(Kernel Function)**是指引入函数$k(x, x')$用来替换特征变换的内积$\phi(x)^T\phi(x')$计算。
- **核方法(Kernel Method)**是指满足表示定理的模型的目标函数和最优解只涉及输入之间的内积，通过引入核函数不需要显式地定义特征空间和映射函数，可以直接获得结果。



## 表示定理 Representer Theorem

我们重点关注线性模型的表示定理，表示定理适合用于任何模型h，只要该模型的优化问题可以构成如下结构风险与经验风险之和：
$$
\underset{h\in \mathcal{H}}{min} \ \Omega(||h||_\mathcal{H}) + \frac{1}{N}\sum^N_{i=1}l(y_i, h(x_i))
$$
其中$\mathcal{H}$为核函数k对应的再生希尔伯特空间，$||h||_{\mathcal{H}}$表示$\mathcal{H}$空间中关于h的范数，要求$\Omega$是单调递增的函数，$l$是非负损失函数，上述优化问题的最优解可以表示为核函数的线性组合：
$$
h^*(x)=\sum^N_{i=1}\alpha_ik(x_i, x)
$$


##  核函数的定义

### Definition 1

对于函数$k:\mathcal{X} \times \mathcal{X} \rightarrow \R$，如果存在映射$\phi:\mathcal{X} \rightarrow \R, \phi\in \mathcal{H}$，使得：
$$
k(x, x')=<\phi(x), \phi(x')>
$$
则称$k(x, x')$为正定核函数。其中$\mathcal{H}$是希尔伯特空间(Hilbert Space)，即完备的、可能是无限维的、被赋予内积的线性空间。

- 完备：对极限是封闭的：$\forall\ Cauchy Sequence:\{x_n\}, \underset{n \rightarrow \infty}{lim} x_n=x\in \mathcal{H}$
- 内积：满足线性、对称性和非负性的内积运算
- 线性空间：对加法和数乘封闭的向量空间



### Definition 2

对于函数$k:\mathcal{X}\times \mathcal{X} \rightarrow \R$，如果满足下面两条性质：

- $k(x, x')$是对称函数，即$k(x, x') = k(x', x)$
- 对任意样本集$x=\{x_1, x_2,...,x_N \}^T$，其中Gram矩阵（核矩阵）$K=[k(x_i, x_j)]_{N\times N}$是半正定矩阵

则称$k(x, x')$为正定核函数。即一个**对称函数所对应的核矩阵半正定，该函数就能够作为核函数**，所以称为正定核函数。



### 两种定义的等价性

#### Proof:

对称性： $k(x, x') = <\phi(x), \phi(x')> = <\phi(x'), \phi(x)> = k(x', x)$

正定性：由正定定义，引入$\alpha\in \R^N$:
$$
\alpha^TK\alpha = [\alpha_1, \alpha_2, ..., \alpha_N][k(x_i, x_j)]_{N\times N}[\alpha_1, \alpha_2, ..., \alpha_N]^T \\
=\sum^N_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_jk(x_i, x_j)=\sum^N_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_j<\phi(x_i), \phi(x_j)> \\
=\sum^N_{i=1}\sum^{N}_{j=1}\alpha_i\alpha_j\phi(x_i)^T\phi(x_j)=\sum^N_{i=1}\alpha_i\phi(x_i)^T\sum^{N}_{j=1}\alpha_j\phi(x_j) \\
=(\sum^N_{i=1}\alpha_i\phi(x_i))^T (\sum^{N}_{j=1}\alpha_j\phi(x_j))=<\sum^N_{i=1}\alpha_i\phi(x_i), \sum^{N}_{j=1}\alpha_j\phi(x_j)> = ||(\sum^N_{i=1}\alpha_i\phi(x_i))||^2 >0
$$

## 常用核函数

对于一个核函数，总能找到一个对应的映射$\phi$，即任何核函数都隐式的顶一个了一个称为**再生核希尔伯特空间**的特征空间，通常希望样本在特征空间中是线性可分的，选择核函数相当于选择特征空间，因此选择合适的核函数是核方法中的重要问题。

### 线性核(Linear Kernel)

线性核函数定义为：
$$
k(x,x')=x^Tx'
$$

- 优点：模型简单，速度快，可解释性好
- 缺点：无法处理线性不可分的数据集