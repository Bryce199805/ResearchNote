# 20240517

---

## 论文阅读  

### Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency and Better Transferability 2022CVPR

> 应用  图像检索  多教师融合
### Let All Be Whitened: Multi-Teacher Distillation for Efficient Visual Retrieval 2024AAAi 
针对图像检索任务提出的多教师蒸馏方法，在进行教师融合时，不同视觉检索模型得到的表示空间是多样化的，其分布具有明显的差异，对于检索模型直接使用融合策略是次优的。这项工作通过白化消除教师差异，白化后的教师模型的余弦相似度将遵循先沟通的分布，这对于多教师蒸馏至关重要。

> 应用  半监督 增量学习
### Semi-Supervised Blind Image Quality Assessment through Knowledge Distillation and Incremental Learning 2024 AAAI
利用标记数据训练一个教师模型，核岭回归获得伪标签，通过增量学习来不断改善性能，在每个增量过程结束后将学生模型设为老师，提出最小差异topK SDK采样来防止灾难性遗忘，在第i个增量阶段，从之前的i-1个阶段中选择具有代表性的例子，合并到第i阶段的数据集中一同学习。

### Understanding the Role of the Projector in Knowledge Distillation 2024AAAI

---

## 代码实践

---

## 项目进展

---

## Mathematics

