# 20240517

---

## 论文阅读  

> 这两项工作都是致力于获得更好的模型表示能力
### Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency and Better Transferability 2022CVPR
这项工作致力于**直接使用预训练的模型**进行蒸馏学习，我们发现在师生模型之间添加可学习模块来匹配师生维度会将表示学习转化为一个间接的过程，损害表示学习性能。我们**提出非参数的对齐方式**，通过奇异值分解SVD来完成师生模型的对齐，SVD会导致通道之间特征方差差异较大，会影响网络的学习优化，提出指数温度缩放PTS来减少方差的差异饼保持原始特征的相对幅度。KDEP表现出更宽的收敛速度，更高效的数据效率和更好的可移植性。

### Knowledge Distillation Meets Self-Supervision 2020ECCV
这项工作**引入自监督**的辅助任务来帮助模型学习更全面的知识，通过对数据集进行变换来创建正负样本对，这个过程是为了获得更好的特征表示而不是为了扩充数据集，为了更好的利用变换样本带来的暗知识，提出了**选择性转移策略**，利用其分类排名作为评价指标，只转移正确知识和topK的知识来避免极端错误对学生模型带来的负面影响。**SSKD的核心在于传递这种不准确但结构化的知识**。
- 引入自监督对比损失来增强模型表示能力
- 引入选择性转移策略来保证这种不准确但结构化的知识不会给学生模型带来负面影响

---

> 这两项工作针对于大模型的蒸馏，都是基于对齐方式的创新
### Asymmetric Masked Distillation for Pre-Training Small Foundation Models 2024CVPR
这项工作针对先前的MAE的重建蒸馏对教师模型的高掩码率会丢失一些重要的结构信息从而导致预训练模型捕获不完整和有偏的视觉信息，这项工作提出了**非对称的MAE蒸馏**，并提出一套对齐方法，结合直接对齐和生成对齐方法来对师生模型特征进行对齐。

### No Head Left Behind - Multi-Head Alignment Distillation for Transformers  2024AAAI
针对多头注意力的蒸馏方法，先前工作一对一的方式没有考虑语义的对齐性，并且不同数量的注意力头不能完美的匹配，这项工作针对这个问题提出了**注意力图对齐蒸馏**，利用教师头和学生头之间的余弦相似性，教师头指导每一个学生头，权重由相似性控制，基于这种思想给出了4中变体的AMAD损失。

---

> 对于蒸馏中组件的研究
### Understanding the Role of the Projector in Knowledge Distillation 2024AAAI
这项工作针对蒸馏中的投影层、归一化层和距离函数进行了探究，将蒸馏中的投影层描述为蒸馏损失本身所需要的信息编码器，从理论层面证明了权重矩阵更新来自于师生模型自相关和互相关的组合，增大投射层的维度并不能显著改善性能，使用线性投射即可；证明了白化的有效性，提出使用batch norm来对特征去相关处理；针对师生模型差距大时性能的下降，归结于学生没有足够的容量来完美对齐教师，提出使用LogSum函数来软化与教师分布的完全对齐
- 投影层的研究，线性投影
- 白化的有效性证明，用BN对特征去相关性
- 使用新的距离函数来弥合师生差距大无法完美对齐的问题，LogSum函数

> 应用  图像检索 多教师融合
### Let All Be Whitened: Multi-Teacher Distillation for Efficient Visual Retrieval 2024AAAi 
针对**图像检索任务**提出的**多教师蒸馏**方法，在进行教师融合时，不同视觉检索模型得到的表示空间是多样化的，其分布具有明显的差异，对于检索模型直接使用融合策略是次优的。这项工作通过**白化消除教师差异**，白化后的教师模型的余弦相似度将遵循相同的分布，这对于多教师蒸馏至关重要。

> 应用  半监督 增量学习
### Semi-Supervised Blind Image Quality Assessment through Knowledge Distillation and Incremental Learning 2024AAAI
利用标记数据训练一个教师模型，**核岭回归获得伪标签**，通过增量学习来不断改善性能，在每个**增量过程结束后将学生模型设为老师**，提出最小差异topK SDK采样来防止灾难性遗忘，在第i个增量阶段，从之前的i-1个阶段中选择具有代表性的例子，合并到第i阶段的数据集中一同学习。

---

## 代码实践

- 写了点实验代码，正在调参
- 准备写几个可视化工具
- 目前想试试能不能用过拟合的思想解释师生模型之间的差距
- 并且想要扩展到ViT上，看看能不能把大模型的知识转移到较小的卷积网络上

---

## 项目进展

- 研究了 hdf5 的c++读写支持
- 阅读cae后端go代码，准备接手后端的工作

---


