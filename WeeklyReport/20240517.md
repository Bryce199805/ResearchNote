# 20240517

---

## 论文阅读  

### Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency and Better Transferability 2022CVPR

---

## 代码实践

---

## 项目进展

---

## Mathematics

