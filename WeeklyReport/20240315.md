# 20240315
## 论文阅读
目前论文阅读集中在早期一些比较经典的蒸馏方法

### Deep Model Compression: Distilling Knowledge from Noisy Teachers 2016arXiv
通过对教师输出的logit添加噪声来模拟同一专业的不同教师，并且证明添加噪声相当于正则化的作用

### Distilling Word Embeddings: An Encoding Approach 2016CIKM CCF-B
提出一个词嵌入的蒸馏，证明了小型嵌入会丢失句法的某些信息,而使用蒸馏嵌入则会保留下来这部分信息，思想比较简单最后的可行性分析值得思考与借鉴。

### Net2Net: Accelerating Learning via Knowledge Transfer 2016ICLR
本文通过将先前的知识从一个小的神经网络快速转移到一个较大的神经网络中，将原网络转移到更深或更宽的网络中，并给出了一套扩展网络权重的转移计算方法，实验证明使用这种方式训练的更大的神经网络可以提高ImageNet的识别性能。

### Face Model Compression by Distilling Knowledge from Neurons 2016AAAI
软标签的蒸馏在人脸识别的压缩DNN上很难收敛，提出在顶部隐藏层神经元中获取额外的教师知识，将神经元选择表述为全连接图上的推理问题，每个节点表示选择的成本，边表示选择的成对成本，目标是选择更具辨别性的神经元而其之间相关性较低。


### Recurrent neural network training with dark knowledge transfer 2016ICASSP CCF-B
在RNN上提出了一种基于知识迁移的训练算法，利用一个简单的DNN模型训练一个复杂的RNN模型，同时使用了软标签和硬标签，并且从正则化和预训练的视角讨论了这一方法

### Knowledge Projection for Effective Design of Thinner and Faster Deep Neural Networks 2017arXiv
提出一种知识投影网络(KPN)，在预训练的大型教师网络的指导下，采用两阶段联合优化的方法来训练小型网络,第一阶段通过任务损失优化投射层和学生网络的下半部分，通过定义的投射损失和投射层输出优化上半部分；第二阶段将第一阶段的结果作为初始化，利用提出了联合损失进行进一步优化达到学生网络内部到最终输出的平稳输出


### Rocket Launching: A Universal and Efficient Framework for Training Well-performing Light Net 2018AAAI (code)
提出了一种称为火箭的训练框架，包括加速网络（教师网络）和轻量级网络（学生模型），两者共享低级别层的参数，且各自都有特定曾用于任务的学习和预测，并提出梯度阻断方案来完善两个网络的优化过程

### Deep Mutual Learning 2018CVPR
提出了互学习的概念，两个或多个学生模型之间互相学习。 并且通过多模型的互学习优化时的不同集成策略证明了具有高熵的后验能够训练出更鲁棒的模型结构。

### Quantization Mimic: Towards Very Tiny CNN for Object Detection 2018ECCV
将量化方法和蒸馏方法相结合提出了一种新的量化蒸馏框架，训练一个全精度教师网络，将其压缩为量化网络，提出一种模仿损失，让学生网络量化后的特征图模拟教师网络量化后的特征图

### Moonshine: Distilling with Cheap Convolutions 2018NeurIPS
本文没有使网络变得更薄更浅，采用此类网络所使用的标准卷积块，并将其为替换为更便宜的卷积块，保持原始架构不变。
分组卷积G(g)：将卷积分成g组，若$N_{in}=N_{out}=N$，则卷积消耗从$N^2k^2$降低到$(\frac{N}{g})^2k^2$，降低了g倍
瓶颈块B(b)：首先将输入通道数降低b倍进行卷积，在最后一个过程恢复到所需要的$N_{out}$

### MSD: Multi-Self-Distillation Learning via Multi-Classifiers within Deep Neural Networks 2019arXiv
与***ICCV2019 Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation***几乎完全一致，实验很少，推测工作落后于别人，冲突后没投  
唯一的区别在于本文认为对于特征层的模仿学习在后期大的权重参数会影响网络的性能，因此引入余弦退火来削弱后期特征损失的权重值

### Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation 2019ICCV (code)
提出了一种自蒸馏的方法，将网络分为多个部分，每个部分都可以进行独立的输出，引入了三种损失：交叉熵损失，用真实标签指导每一个子网学习数据集中的知识；KL损失，让每个子网的softmax输出模仿最深层网络的输出;Hint损失，最小化每个子网与最深分类器的特征图的距离。用改损失来指导网络模型的优化。

### Improved Knowledge Distillation via Teacher Assistant 2020AAAI (code)
这项工作当师生模型之间差距过大时，蒸馏的性能并不好。探究了出现这种问题的原因，并提出了在保持师生模型不变的情况下引入一个助教来弥合这种差距，先用复杂的教师模型训练助教，然后再通过助教训练较小的学生模型，实验证明最佳的助教尺寸是道道师生模型平均性能的尺寸。

### Self-supervised Label Augmentation via Input Transformations 2020PMLR
这项工作发现线性可分的数据经过某些数据增强可能会不在线性可分，提出了一种学习原始标签和自监督标签的统一分布的方法，主要思想是在转换后的样本中除去分类器中不必要的不变属性，并解释了本文的联合分类器与多任务和数据增强的不同点；最后引入了聚合推理和聚合自蒸馏来加快推理的速度。

### ResKD: Residual-Guided Knowledge Distillation 2021 IEEE Transactions on Image Processing
这项工作利用了ResNet的思想，认为师生模型之间的差距可以被视为知识来指导模型的学习，提出了一种残差引导的学习方法，使用一系列的残差学生模型来弥合师生模型之间的差距，残差学生模型由NAS搜索得到合适的结构，并且提出了一种能量熵来衡量模型之间的差异用于停止训练，最后在推理阶段提出一个自适应推理，根据不同难易程度的样本集成适量的残差学生网络来进行更快的推理。

### Unifying Distillation and Privileged Information 2016ICLR
这项工作给出了蒸馏学习从数学层面的解释推理，这项工作的推理在近期论文阅读中出现多次，是对于***An Overview of Statistical Learning Theory 1998***提出的VC理论在蒸馏方法上的解释，98年这篇文章我认为是机器学习在理论数学上比较严格的推理证明我没太看懂，而本文其实更多的我认为是对于实验结果的数学解释，因为其中很多的推理假设是来自于实验和经验的推导，是没有严格的数学证明的，但这不妨碍我们日后借鉴这套理论增强我们自己文章的理论性。

## 代码实践
复现了上述论文中后用***code***标识的文章代码，其中在阅读别人代码中学习了：  
NNI微软调参框架，目前仅了解，还需进一步动手实践  
torchnet.Engine模型训练框架，目前仅了解，还需进一步动手实践
## 项目进展
研究了关于Kubesphere api的相关知识  
研究了代码中向后端发送请求的部分代码  
协助师弟师妹完成项目初期的启动部署工作
