






更好的模型表示



 Knowledge Distillation Meets Self-Supervision 2020ECCV



应用

Let All Be Whitened: Multi-Teacher Distillation for Efficient Visual Retrieval 2024AAAi 

Semi-Supervised Blind Image Quality Assessment through Knowledge Distillation and Incremental Learning 2024AAAI

UniADS: Universal Architecture-Distiller Search for Distillation Gap

M2SD: Multiple Mixing Self-Distillation for Few-Shot Class-Incremental Learning

MIND: Multi-Task Incremental Network Distillation