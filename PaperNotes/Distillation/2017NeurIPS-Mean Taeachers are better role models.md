# Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results

**[NeurIPS 2017](https://proceedings.neurips.cc/paper/2017/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html)	no code 	半监督学习	CIFAR10 SVHN** 

这项工作针对2017ICLR提出的时序集成模型做出了改进，原模型学习到的知识以很慢的速度融入到训练过程中，这导致训练速度较慢，本文通过引入学生模型的EMA权重来逐步更新教师模型的权重，从而加快训练进程。

## Introduction 

先前工作在每个数据点上对添加噪声和不添加噪声的版本进行评估，然后通过最小化一致性损失让其两者输出相似，此时模型具有教师和学生双重角色，如果对作为教师生成的目标赋予了过多的权重，则一致性损失会损害模型的性能。

至少有两种方式可以提高目标质量，一种是仔细选择表征的扰动，而不是勉强的应用加性或乘性噪声给；另一种是谨慎选择教师模型而不是勉强复制学生模型。我们选择第二种方法，这两种方法是兼容的，两者结合可能产生更好的结果，但这项工作不在本文的研究范围之内。

我们的目标是在没有额外训练的情况下，从学生模型中形成更好的教师模型。softmax模型通常不会再训练数据之外提供准确的预测，我们向模型中添加噪声来缓解这一问题；*2017ICLR-Temporal Ensembling*中$\Pi$模型可以通过时序集成进一步改进。然而时序集成么个epoch值更新一次，因此学习到的知识以一个较慢的速度融入到训练过程中，数据集越大，更新的时间就越长。



## Method

![image-20240324165833902](../Base/imgs/image-20240324165833902.png)

为了克服时序集成方法的局限性，我们提出了平均模型权重而不是平均预测结果，由于教师模型是连续学生模型的平均，我们的方法称之均值教师方法。

教师模型不与学生模型共享权重而是使用学生模型的指数移动平均（EMA）权重，现在可以以每一步而不是每一个epoch之后聚合信息，且权重平均改善了所有层的输出而不仅仅是顶层的输出，因此目标模型具有更好的中间表示，改进后的方法具有更准确的目标标签从而有更快的师生模型之间的反馈循环；能够扩展到大型数据集和在线学习中。

我们将一致性成本定义为学生模型（$权重\theta,噪声\eta$）和教师模型（$权重\theta',噪声\eta'$）预测之间的期望距离:
$$
J(\theta) = \mathbb{E}_{x,\eta',\eta}[||f(x,\theta',\eta') - f(x,\theta,\eta)||^2]
$$
$\Pi$模型、时序集成与均值教师模型之间的差异在于如何获得教师模型的预测，$\Pi$模型使用$\theta'=\theta$，时序集成用连续预测的加权平均值近似$f(x,\theta',\eta')$，我们定义训练步t时刻的$\theta’_t$为连续$\theta$权重的EMA：
$$
\theta'_t=\alpha\theta'_{t-1}+(1-\alpha)\theta_t
$$
