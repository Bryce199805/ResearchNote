# Do Deep Nets Really Need to be Deep?

[NeurIPS2014](https://proceedings.neurips.cc/paper_files/paper/2014/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.html)	TIMIT  CIFAR10

**提出了一种新的训练网络的方式，先前利用原始数据来训练网络无法在浅层网络上达到较高的精度，但模型压缩告诉我们较小的网络也能够达到与复杂模型相似的精度，因此提出简单网络模仿已经训练好的复杂网络的logit输出，来达到简单网络实现与深度网络相似的精度，并且在隐藏层的学习中引入了矩阵分解来加快模型的学习。**

## Introduction

训练出向深层网络那样精确的浅层网络是很困难的，如果直接在原始标记数据上训练浅层网络，我们无法达到像深度网络一样的精度，但是如果让一个浅层的网络去模仿一个训练好的深度网络，这样获取能够获得一个高精度的浅层网络。

模型压缩先前的工作表明，一个小型的神经网络原则上是可以学习到更精确的功能，但目前的学习算法无法从原始训练数据中训练出具有该精度的模型，因此我们先训练复杂的中间模型，然后在训练神经网络来模拟它。



## Method

我们对logits进行训练，使得学生模型的学习变得更加容易，同时更能强调教师模型在所有目标上学到的关系，对于一组logits[10, 20, 30]，经过softmax后的概率为[2e-9, 4e-5, 0.9999]，学生模型会专注于第三个目标而忽略前两个目标中的信息，因此直接在logits上训练能够更好的模仿教师模型的详细行为，并且不会在概率空间上发生信息丢失

SNN-MIMIC:
$$
\mathcal{L}(W, \beta) = \frac{1}{2T}\sum_t||g(x^{(t)};W, \beta) - z^{(t)}||^2_2
$$
W是输入与隐藏层之间的权重矩阵，$\beta$是隐藏层到输出层的权值，$g(x^{(t)};W, \beta) = \beta f(Wx^{(t)})$，是第t个训练数据点上的模拟预测，f(·)是隐藏层单元的非线性激活；我们还测试了kl损失和L2概率损失，均不及logit回归损失。我们发现在实验中将Logit进行标注归一化操作能够轻微改善L2损失，但这并不能超过logit损失。



为了匹配深度网络中的参数数量，浅层网络必须在单层中具有更多的非线性隐藏单元，来产生大的权重矩阵，H为隐藏单元数量，D为输入特征维度，学习大小为O(HD)的隐藏层之间的权重矩阵是非常缓慢的，因为其中有许多相关的参数，梯度收敛非常慢。我们发现引入一个具有k个线性隐藏单元的瓶颈线性层可以显著增加学习速度，这相当于将权重矩阵$W\in\mathbb{R}^{H \times D}$分解为两个低秩矩阵$U\in\mathbb{R}^{H \times k}$ 和$V \in \mathbb{R}^{k \times D} $，$k << D,H$ ，且空间复杂度也从O(HD) 降到 O(k(H+D))，新的损失函数可以表示为：
$$
\mathcal{L}(U,V, \beta) = \frac{1}{2T}\sum_t||\beta f(UVx^{(t)}) - z^{(t)}||^2_2
$$
这种矩阵分解在先前工作中得到了探索，显现工作集中在最后输出层的分解，而我们应用于输入层和隐藏层之间来提高收敛速度。



## Discussion

#### Why Mimic Models Can Be More Accurate than Training on Original Labels?

这个问题的答案我们认为有以下可能：

> 如果某些标签有错，教师模型可能会消除其中的一些错误从而使学生更容易学习
>
> 如果原始数据集数据分布中有复杂区域，在给定特征和样本上难以学习，教师模型则能够为学生提供更简单的标签，教师模型能够过滤标签达到降低复杂度的功能
>
> 从0/1硬标签中学习要比从教师的条件概率中学习更为困难，教师模型的软标签的不确定性要比原始的硬标签信息更丰富，能进一步增强模型的能力
>
> 原始目标可能部分依赖于无法作为学习输入的特征，通过教师模型的筛选，消除了对不可用特则的依赖

#### 我们没有找到证据表明浅层模型的容量或表征能力有限，主要的限制似乎来源于学习的方式和正则化项

#### **开发算法直接从原始训练中训练出一个高精度的浅层模型，将是一个重要的贡献**