# Instance Temperature Knowledge Distillation

**[arXiv2407](https://arxiv.org/abs/2407.00115)	[code in github](https://github.com/Zhengbo-Zhang/ITKD)	CIFAR  ImageNet  MSCOCO	20240902**

*Zhengbo Zhang, Yuxi Zhou, Jia Gong, Jun Liu, Zhigang Tu*

针对温度系数的调整，先前方法只考虑了当前的收益而没有考虑未来的潜在收益，本文引入强化学习的思想来考虑更长远的收益，将实例温度系数的设置建模为序列决策过程，将师生模型的性能和学生模型对实例的掌握定义为状态；对温度系数的设置定义为动作；将学生模型的性能表现定义为奖励，并提出一种奖励校准机制来缓解一个批次后才能反馈的奖励延时问题，并通过样本的预测熵来筛选高质量样本进行高质量探索。

## Introduction 

现有的对温度系数的调整仅考虑了当前的收益，没有考虑未来的好处；本文将实例温度调整建模为一个顺序决策任务，并提出一种基于强化学习的方法来解决它。

提出了一种实例奖励校准机制，来应对蒸馏中奖励收益延迟的问题

提出的RLKD可以作为一种即插即用的技术来提高KD算法的性能。

## Method

在调整温度时，先前方法只考虑当前阶段的收益，忽略了未来学习阶段温度调整的潜在回报。

**因此将KD中实例温度的调整视为一个序列决策过程，每个实例温度的调整都可以看作是强化学习中的动作，提出一个强化学习框架解决KD中的温度调整问题。**

### Instance Temperature Adjustment as a Sequential Decision-making Task

- 根据教师学生模型在当前训练实例上的表现来估计状态st
- 在给定当前状态st，在先验经验的指导下，智能体对每个状态-动作进行评估，并对每个训练实例执行温度调整动作at
- 智能体执行动作at后，环境转移到状态st+1，并提供了奖励rt+1
- 智能体根据得到的奖励rt+1 和 当前状态 st+1更新自己的策略

#### State

状态作为智能体的输入，对智能体进行实例温度决策其关键作用，状态从教师模型的表现、学生模型的表现和学生模型对实例的掌握程度来衡量

教师模型和学生模型的表现通过最高置信度输出来衡量：
$$
p_{t/s} = argmax_{i \in [k]}f(x)_{teacher/student}(x)_i
$$
学生模型对实例的掌握程度通过对实例预测的不确定性得分来衡量：
$$
v_{student}(x) = 1 - (f_{student}(x)_{p_s} - \underset{i\in [k] \textbackslash ps}{max}f_{student}(x)_i)
$$
最终将状态State定义为 $(f_{teacher}(x)_{p_t}, f_{student}(x)_{p_s}, v_{student}(x))$

#### Action

动作是对实例温度系数的调整，为了克服在离散动作空间中探索的局限性，提出在连续动作空间中探索实例温度T

我们将动作设计为服从高斯分布，演员（actor 生成动作）生成输出高斯分布的均值方差，从高斯分布中随机采样一个值作为温度系数来提高动作探索的随机性和灵活性。最后我们将温度限制在0-10之间：
$$
\mathcal{T} = 10 \cdot sigmoid(\mathcal{T})
$$

#### Reward

奖励函数提供了智能体行动的质量反馈，来帮助智能体改进行动策略。我们的目的是最大化学生网络的性能，因此我们将两个批次训练中学生网络的性能提升作为奖励，但是训练初期性能提升显著但这并不能反映智能体精明的行动策略，因此需要加以限制：
$$
reward = sigmoid(\mathcal{E}/n) \cdot reward
$$
$\mathcal{E}$表示当前迭代轮数，n为超参数

### Instance Reward Calibration  实例奖励校准

智能体的每个动作存在延迟，假设批次大小为32，则需要在32个操作之后才能得到奖励，这种延迟奖励会使评估和改进变得困难。

因此提出基于奖励分解的改进设计了奖励修正器，奖励修正器根据当前批次的状态sb和agent对每个实例的动作ab来对当前批次的奖励rb进行重新分配。得到修正后的奖励：
$$
r^{b'} = C(s^b, a^b, r^b)
$$
为了考虑每个实例动作对整个批次奖励的贡献，引入一个辅助任务，该任务允许奖励修正器在每个时间步预测整个序列的回报G，损失函数定义为：
$$
L_C = \alpha\cdot(r_n^{b'} - r^b)^2 + \frac{\beta}{n} \cdot \sum^n_{i=1}(r_i^{b_i} - G_i)^2
$$
n表示批次大小，$r_n^{b'}$表示第n次修正的奖励，Gi表示第i个时间步的收益，我们还设计了一个相应的状态更新器：
$$
s^{b'} = U(s^b)
$$
对应的损失函数定义为：
$$
L_U = (\mathcal{E}(s^{b'}) - G)^2
$$
$\mathcal{E}$表示基于输入状态来预测相应回报G的估计量

### Efficient Exploration 高质量探索

在KD的背景下定义高质量数据，先前工作通过预测熵来衡量该实例的知识价值：
$$
H(y|x)=-\sum^C_{c=1}p(y=c|x)logp(y=c|x)
$$
计算所有实例的预测熵，对其降序排列形成序列Se：
$$
S_e= \{I_1, ...,I_n\}
$$
n表示实例总数，提取前20%为高质量样本，并且为了降低过拟合的风险，利用40%-50%的样本mixup到高质量样本上：
$$
S_e^t = \lambda\cdot S_e^{(10-20)\%} + (1-\lambda) S_e^{(40-50)\%}
$$
![image-20240902210441543](imgs\image-20240902210441543.png)