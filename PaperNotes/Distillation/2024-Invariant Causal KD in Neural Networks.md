# Invariant Causal Knowledge Distillation in Neural Networks

2024	[code in github](https://github.com/giakoumoglou/distillers)	CIFAR ImageNet STL-10 TinyImageNet	20241010

本文针对logit蒸馏进行改进，针对先前的对比学习转移结构信息CRD，其忽视了表示学习中的不变性表示，并且需要较大的内存，提出因果不变蒸馏框架，在蒸馏损失中加入对比学习，并引入师生模型的相似性分数来促进学生获得教师的一致性表示。

## Introduction

传统KD最小化师生模型之间概率输出的KL散度，这往往会忽视教师网络中关键的结构信息。

CRD通过对比学习最大化师生模型表征之间的互信息，结合实例级判别和负样本信息库能达到更有效的知识转移，但是CRD依赖于较大的内存且没有显式的在学习表示中执行不变性。

我们提出因果不变蒸馏法(Invariant Causal Distillation, ICD)借鉴了CRD的思想同时解决了其局限性。

- ICD 一种新的KD框架，将对比学习与显式的不变惩罚相结合，在不需要内存缓冲的情况下，确保师生模型之间一致和稳健的表示。
- 我们证实了ICD的有效性，在多个数据集上的结果表明其准确性和鲁棒性都有显著提升

## Method

#### 问题描述

X表示可用数据，$Y = \{ Y_t\}^T_{t=1}$为一组位置的下游任务，Yt表示任务t的目标，一个高容量的教师神经网络模型$f^T$，将知识传递给一个更紧凑的学生模型$f^S$，目标不仅需要模拟教师模型的表现，还要确保学生模型学习到鲁棒的泛化的表示。

#### 理论基础

##### 因果推理实现不变表示

利用下游任务Yt来生成共同假设，图像x由上下文和风格变量生成，只有上下文关于Yt是相关的，上下文C和风格S相互独立，在我们的因果模型中，上下文C和风格S共同生成图像x，C直接影响Yt而S不影响，根据**独立机制原则**，在该因果模型中，对S的干预不会影响条件分布$P(Y_t|C)$，称为$P(Y_t|C)$在风格S变化下是不变的，C为Yt在S条件下的不变表示
$$
P(T_t|C)_{S=s_i} = P(T_t|C)_{S=s_j}\ \forall s_i, s_j = S
$$

##### 模型差异实现不变表示

由于目标Yt是未知的，创建一个代理任务$Y^R$，仅从数据X中学习表示，为了将不变表示从教师$f^T$转移到学生$f^S$中，我们目标是不改变风格的前提转移内容信息，因此必须有：
$$
P(Y^R|f^S(X))_{S=s_i} = P(Y^R|f^T(X))_{S=s_j}
$$
由于S和C是潜在的，不能直接访问，因此：

- 假设教师模型已经学习到了一个能够捕获上下文且风格不变的表示
- $f^T(X), f^S(X)$之间的差异可以归因于风格的变化，因为个模型能够处理相同的输入

基于以上分析，我们的优化目标：
$$
E_{x_i}\{E_{s_i, s_j}[\sum_{s\in \{s_i, s_j\}} L_s(Y^R, f^S(X) - f^T(X))] \}  \\
s.t.\ KL[P(Y^R|f^T(X))_{S=s_i}, P(Y^R|f^S(X))_{S=s_j}] \leq \rho
$$
这允许我们对师生模型如何解释风格信息的潜在差异进行建模，该约束保证了代理任务在师生模型之间保持相似，在允许风格差异的前提下有效的进行知识传递，

##### 用于知识对齐的对比实例判别

我们为每一个样本点xi分配唯一的标识符，$y^R_i = i$,这反映了对比学习中流行的判别方法，令$f^S(x_i)$与$f^T(x_i)$相似来加强判别性，定义：
$$
P(Y^R = j | f^S(x_i)) \propto exp(\phi(f^S(x_i), f^T(x_i))/\tau)
$$
$\phi$为相似函数。

### 结合对比学习和不变表示

$$
L_{KD}(z^T_i, z^S_i) = L_{contrast}(z^T_i, z^S_i) + \alpha·L_{invariance}(z^T_i, z^S_i)
$$

$\alpha$为平衡对比损失与不变性惩罚的超参数。

对比损失促使师生模型对相同输入数据表征相似而对不同输入数据表征疏远：
$$
L_{contrast}(z^T_i, z^S_i) = -log\frac{exp(\phi(z^T_i, z^S_i)/\tau)}{\sum^M_{j=1}exp(\phi(z^T_i, z^S_i)/\tau)}
$$
M表示负样本的个数，从批次样本中采样，相似函数描述为：
$$
\phi(z^T_i, z^S_i) = \frac{z^T_i · z^S_i}{||z^T_i||||z^S_i||}
$$
为了加强不变性，我们最小化$z^T_i, z^S_i$之间的差异，才促进学习不变表示：
$$
L_{invariance}(z^T_i, z^S_i) = D_{KL}(p(z^T_i) || p(z^S_i))
$$
总体损失表示为：
$$
L = L_{sup}(y_i, z^S_i) + \lambda L_{distill}(z^T_i, z^S_i) + \beta L_{KD}(z^T_i, z^S_i)
$$
